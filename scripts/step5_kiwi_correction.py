#!/usr/bin/env python3
"""
â‘¤ 1ì°¨ êµì • (í˜•íƒœì†Œ ë¶„ì„)
ì…ë ¥: layout_combined/doc1_sections.json
ì²˜ë¦¬: Kiwië¡œ ë„ì–´ì“°ê¸°, ìëª¨ ë¶„ë¦¬/í•©ì¹¨, ê°„ë‹¨ ì¡°ì‚¬ ì˜¤ë¥˜ ë³´ì •
ì¶œë ¥: corrections/kiwi_intermediate.json
"""

import json
import re
from pathlib import Path
from datetime import datetime
from itertools import product

# ì„¤ì •
BASE_DIR = Path(__file__).parent.parent
CURRENT_SESSION_DIR = None

# Kiwi ì´ˆê¸°í™” (lazy loading)
kiwi = None
KIWI_AVAILABLE = False

# í•œê¸€ ìëª¨ ì •ì˜
CHOSUNG_LIST = ['ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
JUNGSUNG_LIST = ['ã…', 'ã…', 'ã…‘', 'ã…’', 'ã…“', 'ã…”', 'ã…•', 'ã…–', 'ã…—', 'ã…˜', 'ã…™', 'ã…š', 'ã…›', 'ã…œ', 'ã…', 'ã…', 'ã…Ÿ', 'ã… ', 'ã…¡', 'ã…¢', 'ã…£']
JONGSUNG_LIST = ['', 'ã„±', 'ã„²', 'ã„³', 'ã„´', 'ã„µ', 'ã„¶', 'ã„·', 'ã„¹', 'ã„º', 'ã„»', 'ã„¼', 'ã„½', 'ã„¾', 'ã„¿', 'ã…€', 'ã…', 'ã…‚', 'ã…„', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']

# ììŒê³¼ ëª¨ìŒ ì§‘í•©
CONSONANTS = set('ã„±ã„²ã„´ã„·ã„¸ã„¹ã…ã…‚ã…ƒã……ã…†ã…‡ã…ˆã…‰ã…Šã…‹ã…Œã…ã…ã„³ã„µã„¶ã„ºã„»ã„¼ã„½ã„¾ã„¿ã…€ã…„')
VOWELS = set('ã…ã…ã…‘ã…’ã…“ã…”ã…•ã…–ã…—ã…˜ã…™ã…šã…›ã…œã…ã…ã…Ÿã… ã…¡ã…¢ã…£')

def init_kiwi():
    """Kiwi ì´ˆê¸°í™”"""
    global kiwi, KIWI_AVAILABLE
    if kiwi is not None:
        return True
    
    try:
        from kiwipiepy import Kiwi
        kiwi = Kiwi()
        KIWI_AVAILABLE = True
        print("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ")
        return True
    except Exception as e:
        KIWI_AVAILABLE = False
        print(f"âš ï¸ Kiwi ì‚¬ìš© ë¶ˆê°€: {e}")
        return False

def decompose_hangul(char):
    """í•œê¸€ ê¸€ìë¥¼ ì´ˆì„±, ì¤‘ì„±, ì¢…ì„±ìœ¼ë¡œ ë¶„í•´"""
    if not ('ê°€' <= char <= 'í£'):
        return None
    
    code = ord(char) - 0xAC00
    cho_idx = code // 588
    jung_idx = (code % 588) // 28
    jong_idx = code % 28
    
    return (CHOSUNG_LIST[cho_idx], JUNGSUNG_LIST[jung_idx], JONGSUNG_LIST[jong_idx])

def compose_hangul(cho, jung, jong=''):
    """ì´ˆì„±, ì¤‘ì„±, ì¢…ì„±ì„ í•©ì³ì„œ í•œê¸€ ê¸€ì ìƒì„±"""
    try:
        cho_idx = CHOSUNG_LIST.index(cho)
        jung_idx = JUNGSUNG_LIST.index(jung)
        jong_idx = JONGSUNG_LIST.index(jong)
        
        code = 0xAC00 + (cho_idx * 588) + (jung_idx * 28) + jong_idx
        return chr(code)
    except (ValueError, IndexError):
        return None

def is_jamo_separated(text):
    """ìëª¨ê°€ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸ (ì˜ˆ: 'ã…ã…ã„´ã„±ã…¡ã„¹')"""
    if not text:
        return False
    
    # ìëª¨ ë¬¸ìê°€ 2ê°œ ì´ìƒ ì—°ì†ë˜ë©´ ë¶„ë¦¬ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
    jamo_count = sum(1 for c in text if c in CONSONANTS or c in VOWELS)
    return jamo_count >= 2

def try_compose_jamo_sequence(jamo_text):
    """ë¶„ë¦¬ëœ ìëª¨ ë¬¸ìì—´ì„ ê°€ëŠ¥í•œ í•œê¸€ ì¡°í•©ìœ¼ë¡œ ë³€í™˜
    ì˜ˆ: 'ã…ã…ã„´ã„±ã…¡ã„¹' -> ['í•œê¸€', 'í•¨ê¸€', 'í•­ê¸€', ...]
    """
    if not jamo_text:
        return []
    
    candidates = []
    
    # ìëª¨ë§Œ ì¶”ì¶œ
    jamos = [c for c in jamo_text if c in CONSONANTS or c in VOWELS]
    if len(jamos) < 2:
        return []
    
    # ê°€ëŠ¥í•œ ì¡°í•© ìƒì„± (ìµœëŒ€ 3ê¸€ìê¹Œì§€ë§Œ ì‹œë„)
    def generate_combinations(jamos, max_attempts=50):
        results = []
        
        def recurse(remaining, current_result, attempt_count):
            if attempt_count > max_attempts:
                return
            
            if not remaining:
                if current_result:
                    results.append(''.join(current_result))
                return
            
            # ë°©ë²• 1: ì´ˆì„±+ì¤‘ì„± ì¡°í•©
            if len(remaining) >= 2:
                if remaining[0] in CONSONANTS and remaining[1] in VOWELS:
                    char = compose_hangul(remaining[0], remaining[1])
                    if char:
                        recurse(remaining[2:], current_result + [char], attempt_count + 1)
            
            # ë°©ë²• 2: ì´ˆì„±+ì¤‘ì„±+ì¢…ì„± ì¡°í•©
            if len(remaining) >= 3:
                if remaining[0] in CONSONANTS and remaining[1] in VOWELS and remaining[2] in CONSONANTS:
                    char = compose_hangul(remaining[0], remaining[1], remaining[2])
                    if char:
                        recurse(remaining[3:], current_result + [char], attempt_count + 1)
            
            # ë°©ë²• 3: ê·¸ëƒ¥ ë„˜ì–´ê°€ê¸°
            if remaining:
                recurse(remaining[1:], current_result + [remaining[0]], attempt_count + 1)
        
        recurse(jamos, [], 0)
        return results
    
    candidates = generate_combinations(jamos)
    
    # ê²°ê³¼ í•„í„°ë§: ìµœì†Œ í•œê¸€ ë¹„ìœ¨ í™•ì¸
    valid_candidates = []
    for candidate in candidates:
        hangul_count = sum(1 for c in candidate if 'ê°€' <= c <= 'í£')
        if hangul_count >= len(candidate) * 0.5:  # ìµœì†Œ 50% í•œê¸€
            valid_candidates.append(candidate)
    
    return valid_candidates[:10]  # ìµœëŒ€ 10ê°œ í›„ë³´

def calculate_kiwi_score(text):
    """Kiwi í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ í…ìŠ¤íŠ¸ì˜ ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜ ê³„ì‚°"""
    if not text or not KIWI_AVAILABLE:
        return 0.0
    
    try:
        result = kiwi.analyze(text)
        if not result or len(result) == 0:
            return 0.0
        
        # Kiwi ê²°ê³¼ëŠ” [[(Token, Token, ...), score], ...] í˜•íƒœ
        token_list = result[0][0] if isinstance(result[0], tuple) else result[0]
        
        if not token_list:
            return 0.0
        
        total_tokens = len(token_list)
        if total_tokens == 0:
            return 0.0
        
        unk_count = 0
        meaningful_count = 0
        meaningful_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'MAG', 'MM']
        
        for token in token_list:
            # Token ê°ì²´ì¸ ê²½ìš°
            if hasattr(token, 'tag'):
                tag = str(token.tag)
            # íŠœí”Œì´ë‚˜ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            elif isinstance(token, (tuple, list)) and len(token) >= 2:
                tag = str(token[1])
            else:
                continue
            
            # UNK(ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´) ì¹´ìš´íŠ¸
            if tag in ['UNK', 'UNKNOWN']:
                unk_count += 1
            
            # ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬ ì¹´ìš´íŠ¸
            if tag in meaningful_tags:
                meaningful_count += 1
        
        # ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ì›€)
        unk_ratio = unk_count / total_tokens
        meaningful_ratio = meaningful_count / total_tokens
        score = (1.0 - unk_ratio) * 0.7 + meaningful_ratio * 0.3
        
        return score
    except Exception:
        return 0.0

def select_best_candidate_with_kiwi(candidates, context=""):
    """Kiwië¡œ ì—¬ëŸ¬ í›„ë³´ ì¤‘ ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ê²ƒ ì„ íƒ"""
    if not candidates:
        return None
    
    if len(candidates) == 1:
        return candidates[0]
    
    if not KIWI_AVAILABLE:
        # Kiwiê°€ ì—†ìœ¼ë©´ ê°€ì¥ ê¸´ ê²ƒ ì„ íƒ
        return max(candidates, key=lambda x: len([c for c in x if 'ê°€' <= c <= 'í£']))
    
    # ê° í›„ë³´ì˜ ì ìˆ˜ ê³„ì‚°
    scored_candidates = []
    for candidate in candidates:
        # ë¬¸ë§¥ê³¼ í•¨ê»˜ ì ìˆ˜ ê³„ì‚°
        if context:
            full_text = f"{context} {candidate}"
            score = calculate_kiwi_score(full_text)
        else:
            score = calculate_kiwi_score(candidate)
        
        scored_candidates.append((candidate, score))
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ í›„ë³´ ì„ íƒ
    if scored_candidates:
        best_candidate = max(scored_candidates, key=lambda x: x[1])
        return best_candidate[0]
    
    return candidates[0]

def fix_mixed_english_in_korean(text, debug=False):
    """í•œê¸€+ì˜ì–´ í˜¼í•© íŒ¨í„´ ê°ì§€ (ì‹¤ì œ OCR ê²°ê³¼ ê¸°ë°˜)
    
    í˜„ì¬ëŠ” íŒ¨í„´ë§Œ ê°ì§€í•˜ê³  ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‹¤ì œ OCR ê²°ê³¼ë¥¼ ëª¨ì€ í›„ ê°œì„ í•  ì˜ˆì •ì…ë‹ˆë‹¤.
    """
    if not text:
        return text
    
    # íŒ¨í„´ ê°ì§€ë§Œ ìˆ˜í–‰ (êµì •ì€ í•˜ì§€ ì•ŠìŒ)
    # í•œê¸€ + ì˜ì–´ê°€ ì„ì¸ íŒ¨í„´ ì°¾ê¸°
    mixed_pattern = r'([ê°€-í£]+)([a-zA-Z]{2,})([ê°€-í£]*)'
    matches = re.findall(mixed_pattern, text)
    
    if matches and debug:
        print(f"  ğŸ” í•œê¸€+ì˜ì–´ í˜¼í•© íŒ¨í„´ ê°ì§€:")
        for korean, english, suffix in matches:
            print(f"    - {korean}{english}{suffix}")
    
    # ì¼ë‹¨ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì‹¤ì œ OCR ê²°ê³¼ë¥¼ ë¨¼ì € ë´ì•¼ í•¨)
    return text

def normalize_spacing(text):
    """ê¸°ë³¸ ê³µë°± ì •ê·œí™”"""
    # ì—°ì† ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # ë¬¸ì¥ë¶€í˜¸ ì•ë’¤ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s*([,.!?;:])\s*', r'\1 ', text)
    
    # ê´„í˜¸ ì•ˆìª½ ê³µë°± ì œê±°
    text = re.sub(r'\(\s+', '(', text)
    text = re.sub(r'\s+\)', ')', text)
    
    return text.strip()

def fix_jamo_separation(text):
    """ìëª¨ ë¶„ë¦¬/í•©ì¹¨ ì˜¤ë¥˜ ìˆ˜ì • (ê³ ê¸‰ ë¬¸ë§¥ ê¸°ë°˜)"""
    if not text:
        return text
    
    # 1. ê°„ë‹¨í•œ ê³µë°± ë¶„ë¦¬ íŒ¨í„´ ë¨¼ì € ì²˜ë¦¬
    jamo_fixes = {
        'ã„± ã…': 'ê°€', 'ã„´ ã…': 'ë‚˜', 'ã„· ã…': 'ë‹¤', 'ã„¹ ã…': 'ë¼',
        'ã… ã…': 'ë§ˆ', 'ã…‚ ã…': 'ë°”', 'ã…… ã…': 'ì‚¬', 'ã…‡ ã…': 'ì•„',
        'ã…ˆ ã…': 'ì', 'ã…Š ã…': 'ì°¨', 'ã…‹ ã…': 'ì¹´', 'ã…Œ ã…': 'íƒ€',
        'ã… ã…': 'íŒŒ', 'ã… ã…': 'í•˜',
        
        'ã„± ã…“': 'ê±°', 'ã„´ ã…“': 'ë„ˆ', 'ã„· ã…“': 'ë”', 'ã„¹ ã…“': 'ëŸ¬',
        'ã… ã…“': 'ë¨¸', 'ã…‚ ã…“': 'ë²„', 'ã…… ã…“': 'ì„œ', 'ã…‡ ã…“': 'ì–´',
        
        'ã„± ã…—': 'ê³ ', 'ã„´ ã…—': 'ë…¸', 'ã„· ã…—': 'ë„', 'ã„¹ ã…—': 'ë¡œ',
        'ã… ã…—': 'ëª¨', 'ã…‚ ã…—': 'ë³´', 'ã…… ã…—': 'ì†Œ', 'ã…‡ ã…—': 'ì˜¤',
        
        'ã„± ã…œ': 'êµ¬', 'ã„´ ã…œ': 'ëˆ„', 'ã„· ã…œ': 'ë‘', 'ã„¹ ã…œ': 'ë£¨',
        'ã… ã…œ': 'ë¬´', 'ã…‚ ã…œ': 'ë¶€', 'ã…… ã…œ': 'ìˆ˜', 'ã…‡ ã…œ': 'ìš°',
    }
    
    for wrong, correct in jamo_fixes.items():
        text = text.replace(wrong, correct)
    
    # 2. ë³µì¡í•œ ìëª¨ ë¶„ë¦¬ íŒ¨í„´ ì°¾ì•„ì„œ ì²˜ë¦¬ (ê³µë°± ì—†ì´ ë¶™ì–´ìˆëŠ” ê²½ìš°)
    # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬
    words = text.split()
    fixed_words = []
    
    for word in words:
        # ìëª¨ê°€ ë¶„ë¦¬ëœ ë‹¨ì–´ì¸ì§€ í™•ì¸
        if is_jamo_separated(word):
            # ê°€ëŠ¥í•œ ì¡°í•© ìƒì„±
            candidates = try_compose_jamo_sequence(word)
            if candidates:
                # Kiwië¡œ ìµœì  í›„ë³´ ì„ íƒ
                best = select_best_candidate_with_kiwi(candidates, context=' '.join(fixed_words[-3:]))
                if best:
                    fixed_words.append(best)
                    continue
        
        fixed_words.append(word)
    
    return ' '.join(fixed_words)

def analyze_word_context_pattern(text, word_position):
    """ì£¼ë³€ ë‹¨ì–´ì˜ í’ˆì‚¬ íŒ¨í„´ ë¶„ì„"""
    if not KIWI_AVAILABLE:
        return {}
    
    try:
        result = kiwi.analyze(text)
        if not result or len(result) == 0:
            return {}
        
        token_list = result[0][0] if isinstance(result[0], tuple) else result[0]
        
        # ì£¼ë³€ ë‹¨ì–´ì˜ í’ˆì‚¬ ì¶”ì¶œ
        pos_tags = []
        for token in token_list:
            if hasattr(token, 'tag'):
                pos_tags.append(str(token.tag))
            elif isinstance(token, (tuple, list)) and len(token) >= 2:
                pos_tags.append(str(token[1]))
        
        # ë™ì‚¬, ëª…ì‚¬ ë¹„ìœ¨ ê³„ì‚°
        verb_count = sum(1 for tag in pos_tags if tag in ['VV', 'VA', 'VX', 'VCP', 'VCN'])
        noun_count = sum(1 for tag in pos_tags if tag in ['NNG', 'NNP', 'NNB'])
        
        return {
            'pos_tags': pos_tags,
            'verb_count': verb_count,
            'noun_count': noun_count,
            'verb_ratio': verb_count / len(pos_tags) if pos_tags else 0
        }
    except Exception:
        return {}

def correct_context_based_words(text, debug=False):
    """ë¬¸ë§¥ ê¸°ë°˜ ë‹¨ì–´ êµì • (í’ˆì‚¬ íŒ¨í„´ + í˜•íƒœì†Œ ë¶„ì„)
    
    ì‹¤ì œ ë‹¨ì–´ì´ì§€ë§Œ OCR ì˜¤ë¥˜ë¡œ ë‹¤ë¥¸ ë‹¨ì–´ì¼ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ê²½ìš°,
    ë¬¸ë§¥ì˜ í’ˆì‚¬ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì–´ëŠ ìª½ì´ ë” ì í•©í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.
    """
    if not text or not KIWI_AVAILABLE:
        return text
    
    # OCRì—ì„œ ìì£¼ í˜¼ë™ë˜ëŠ” ë‹¨ì–´ ìŒê³¼ íŒ¨í„´
    # (ì›ë³¸, í›„ë³´, í›„ë³´_ì„ í˜¸_í’ˆì‚¬_íŒ¨í„´)
    ambiguous_pairs = [
        {
            'original': 'ë¬´ë¦„',
            'candidate': 'ë¬´ë¦',
            # "ë¬´ë¦"ì€ ì‹ ì²´ ë¶€ìœ„ë¡œ ë™ì‘ ë™ì‚¬(êµ¬ë¶€ë¦¬ë‹¤, í´ë‹¤, ê¿‡ë‹¤ ë“±)ì™€ ìì£¼ ì“°ì„
            # ë™ì‚¬ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ "ë¬´ë¦"ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
            'pattern': lambda ctx: ctx.get('verb_ratio', 0) > 0.25  # ë™ì‚¬ê°€ 25% ì´ìƒ
        },
        # ì—¬ê¸°ì— ë‹¤ë¥¸ í˜¼ë™ ë‹¨ì–´ ìŒ ì¶”ê°€ ê°€ëŠ¥
    ]
    
    words = text.split()
    fixed_words = []
    
    for i, word in enumerate(words):
        # ì£¼ë³€ ë¬¸ë§¥ ê°€ì ¸ì˜¤ê¸° (ì•ë’¤ 7ë‹¨ì–´)
        context_start = max(0, i - 7)
        context_end = min(len(words), i + 8)
        context_text = ' '.join(words[context_start:context_end])
        
        replaced = False
        for pair in ambiguous_pairs:
            original = pair['original']
            candidate = pair['candidate']
            pattern_check = pair['pattern']
            
            if original in word:
                # ë¬¸ë§¥ì˜ í’ˆì‚¬ íŒ¨í„´ ë¶„ì„
                context_pattern = analyze_word_context_pattern(context_text, i - context_start)
                
                # íŒ¨í„´ì´ í›„ë³´ì— ë§ìœ¼ë©´ êµì²´ ê³ ë ¤
                if pattern_check(context_pattern):
                    candidate_word = word.replace(original, candidate)
                    
                    # ì¶”ê°€ ê²€ì¦: í˜•íƒœì†Œ ì ìˆ˜ë„ í™•ì¸
                    original_context = ' '.join(
                        words[context_start:i] + [word] + words[i+1:context_end]
                    )
                    candidate_context = ' '.join(
                        words[context_start:i] + [candidate_word] + words[i+1:context_end]
                    )
                    
                    score_original = calculate_kiwi_score(original_context)
                    score_candidate = calculate_kiwi_score(candidate_context)
                    
                    if debug:
                        print(f"  [{original} vs {candidate}]")
                        print(f"    í’ˆì‚¬ íŒ¨í„´: ë™ì‚¬ë¹„ìœ¨={context_pattern.get('verb_ratio', 0):.2f}, ì¡°ê±´ì¶©ì¡±={'âœ“' if pattern_check(context_pattern) else 'âœ—'}")
                        print(f"    í˜•íƒœì†Œ ì ìˆ˜: ì›ë³¸={score_original:.3f}, í›„ë³´={score_candidate:.3f}, ì°¨ì´={score_candidate - score_original:.3f}")
                    
                    # íŒ¨í„´ì´ ë§ê³  ì ìˆ˜ê°€ í¬ê²Œ ë‚˜ë¹ ì§€ì§€ ì•Šìœ¼ë©´ êµì²´
                    if score_candidate >= score_original - 0.02:  # í›„ë³´ê°€ ì•½ê°„ ë‚®ì•„ë„ OK
                        if debug:
                            print(f"    â†’ {candidate}ë¡œ êµì²´")
                        fixed_words.append(candidate_word)
                        replaced = True
                        break
                    else:
                        if debug:
                            print(f"    â†’ ì ìˆ˜ ì°¨ì´ë¡œ ìœ ì§€")
                
                if not replaced:
                    fixed_words.append(word)
                    replaced = True
                break
        
        if not replaced:
            fixed_words.append(word)
    
    return ' '.join(fixed_words)

def correct_common_ocr_errors(text):
    """ì¼ë°˜ì ì¸ OCR ì˜¤ë¥˜ ìˆ˜ì • (ëª…í™•í•œ ì˜¤ë¥˜ë§Œ)"""
    corrections = {
        # ìˆ«ì/ë¬¸ì í˜¼ë™
        '0': 'O', '1': 'l', '5': 'S',
        
        # ëª…ë°±í•œ OCR ì˜¤ë¥˜ (ì‹¤ì œ ë‹¨ì–´ê°€ ì•„ë‹Œ ê²ƒë“¤)
        'ë¬´abar': 'ë¬´ë¦', 'ë¬´bar': 'ë¬´ë¦',
        # ì£¼ì˜: 'ë¬´ë¦„'ì€ ì‹¤ì œ ë‹¨ì–´ì´ë¯€ë¡œ ë¬¸ë§¥ ê¸°ë°˜ êµì •ìœ¼ë¡œ ì´ë™
        
        # ë‹¨ìœ„ ì •ë¦¬
        'Â°': 'ë„', 'â„ƒ': 'ë„', '%': 'í¼ì„¼íŠ¸',
        
        # ê³µë°± ì˜¤ë¥˜
        '90 Â°': '90ë„', '90Â°': '90ë„',
        '1 0': '10', '2 0': '20', '3 0': '30',
    }
    
    for wrong, correct in corrections.items():
        text = text.replace(wrong, correct)
    
    return text

def kiwi_spacing_correction(text):
    """Kiwië¥¼ ì‚¬ìš©í•œ ë„ì–´ì“°ê¸° êµì •"""
    if not KIWI_AVAILABLE:
        return text
    
    try:
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r'[.!?]\s*', text)
        corrected_sentences = []
        
        for sentence in sentences:
            if len(sentence.strip()) < 3:
                corrected_sentences.append(sentence)
                continue
            
            # Kiwi ë¶„ì„
            result = kiwi.analyze(sentence.strip())
            if result and len(result) > 0:
                # í˜•íƒœì†Œ ê¸°ë°˜ ë„ì–´ì“°ê¸° ì¬êµ¬ì„±
                tokens = []
                
                # Kiwi ê²°ê³¼ëŠ” [[(Token, Token, ...), score], ...] í˜•íƒœ
                # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ì‚¬ìš©
                token_list = result[0][0] if isinstance(result[0], tuple) else result[0]
                
                for token in token_list:
                    # Token ê°ì²´ì¸ ê²½ìš°
                    if hasattr(token, 'form') and hasattr(token, 'tag'):
                        form = token.form
                        tag = str(token.tag)
                    # íŠœí”Œì´ë‚˜ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                    elif isinstance(token, (tuple, list)) and len(token) >= 2:
                        form = str(token[0])
                        tag = str(token[1])
                    else:
                        continue
                    
                    # ì¡°ì‚¬ëŠ” ì• ë‹¨ì–´ì™€ ë¶™ì„
                    if tag.startswith('J'):  # ì¡°ì‚¬
                        if tokens:
                            tokens[-1] += form
                        else:
                            tokens.append(form)
                    else:
                        tokens.append(form)
                
                if tokens:
                    corrected = ' '.join(tokens)
                    corrected_sentences.append(corrected)
                else:
                    corrected_sentences.append(sentence)
            else:
                corrected_sentences.append(sentence)
        
        return '. '.join(corrected_sentences).strip()
        
    except Exception as e:
        print(f"âš ï¸ Kiwi êµì • ì‹¤íŒ¨: {e}")
        return text

def correct_section_text(text):
    """ì„¹ì…˜ í…ìŠ¤íŠ¸ ì¢…í•© êµì • (ë¬¸ë§¥ ê¸°ë°˜ ìëª¨ ë³µì› í¬í•¨)"""
    if not text or len(text.strip()) < 3:
        return text
    
    # 1. ê¸°ë³¸ ì •ê·œí™”
    corrected = normalize_spacing(text)
    
    # 2. í•œê¸€+ì˜ì–´ í˜¼í•© íŒ¨í„´ ìˆ˜ì • (ì˜ˆ: "ë¬´lesë¥¼" -> "ë¬´ë¦ì„")
    corrected = fix_mixed_english_in_korean(corrected)
    
    # 3. ìëª¨ ë¶„ë¦¬ ìˆ˜ì • (ë¬¸ë§¥ ê¸°ë°˜, Kiwi í™œìš©)
    corrected = fix_jamo_separation(corrected)
    
    # 4. ì¼ë°˜ì ì¸ OCR ì˜¤ë¥˜ ìˆ˜ì • (ëª…í™•í•œ ì˜¤ë¥˜ë§Œ)
    corrected = correct_common_ocr_errors(corrected)
    
    # 5. ë¬¸ë§¥ ê¸°ë°˜ ë‹¨ì–´ êµì • (ì˜ˆ: "ë¬´ë¦„" vs "ë¬´ë¦" ë¬¸ë§¥ìœ¼ë¡œ íŒë‹¨)
    corrected = correct_context_based_words(corrected)
    
    # 6. Kiwi ë„ì–´ì“°ê¸° êµì •
    corrected = kiwi_spacing_correction(corrected)
    
    return corrected

def process_sections(sections_data):
    """ì„¹ì…˜ë³„ í…ìŠ¤íŠ¸ êµì •"""
    if not init_kiwi():
        print("âš ï¸ Kiwi ì—†ì´ ê¸°ë³¸ êµì •ë§Œ ìˆ˜í–‰")
    
    corrected_sections = []
    total_fixed_jamo = 0
    total_fixed_mixed = 0
    
    for i, section in enumerate(sections_data, 1):
        print(f"[{i}/{len(sections_data)}] ì„¹ì…˜ êµì •: {section['title'][:30]}...")
        
        original_text = section.get('text_combined', '')
        corrected_text = correct_section_text(original_text)
        
        # êµì • ê²°ê³¼ ì €ì¥
        corrected_section = section.copy()
        corrected_section['text_combined'] = corrected_text
        corrected_section['correction_applied'] = True
        corrected_section['correction_method'] = 'kiwi_morpheme_with_jamo_context'
        
        # ë³€ê²½ ì‚¬í•­ ê¸°ë¡ ë° ë¶„ì„
        if corrected_text != original_text:
            corrected_section['text_changed'] = True
            
            # ìëª¨ ë¶„ë¦¬ ìˆ˜ì • ì—¬ë¶€ í™•ì¸
            has_jamo_orig = any(c in CONSONANTS or c in VOWELS for c in original_text)
            has_jamo_corr = any(c in CONSONANTS or c in VOWELS for c in corrected_text)
            if has_jamo_orig and not has_jamo_corr:
                total_fixed_jamo += 1
                print(f"    âœ¨ ìëª¨ ë³µì›: {len(original_text)} â†’ {len(corrected_text)}ì")
            
            # ì˜ì–´ í˜¼í•© ìˆ˜ì • ì—¬ë¶€ í™•ì¸
            has_mixed_pattern = bool(re.search(r'[ê°€-í£]+[a-zA-Z]{2,6}[ê°€-í£]', original_text))
            if has_mixed_pattern:
                total_fixed_mixed += 1
                print(f"    ğŸ”§ í˜¼í•© íŒ¨í„´ ìˆ˜ì •: '{original_text[:50]}...' â†’ '{corrected_text[:50]}...'")
            else:
                print(f"    âœï¸ êµì •ë¨: {len(original_text)} â†’ {len(corrected_text)}ì")
        else:
            corrected_section['text_changed'] = False
            print(f"    â„¹ï¸ ë³€ê²½ ì—†ìŒ")
        
        corrected_sections.append(corrected_section)
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š êµì • í†µê³„:")
    print(f"   â€¢ ìëª¨ ë³µì›: {total_fixed_jamo}ê±´")
    print(f"   â€¢ í˜¼í•© íŒ¨í„´ ìˆ˜ì •: {total_fixed_mixed}ê±´")
    
    return corrected_sections

def main():
    print("=" * 60)
    print("ğŸ”¤ 1ì°¨ êµì • (í˜•íƒœì†Œ ë¶„ì„ + ë¬¸ë§¥ ê¸°ë°˜ ìëª¨ ë³µì›)")
    print("=" * 60)
    
    # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì½ê¸°
    session_file = BASE_DIR / "current_session.json"
    if not session_file.exists():
        print("âŒ í˜„ì¬ ì„¸ì…˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    with open(session_file, 'r', encoding='utf-8') as f:
        session_info = json.load(f)
    
    global CURRENT_SESSION_DIR
    CURRENT_SESSION_DIR = Path(session_info['session_dir'])
    
    # step4 ê²°ê³¼ íŒŒì¼ ë¡œë“œ
    step4_dir = CURRENT_SESSION_DIR / "step4_layout"
    section_files = list(step4_dir.glob("*_sections.json"))
    if not section_files:
        print(f"âŒ ì„¹ì…˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {step4_dir}")
        return
    
    section_file = section_files[0]
    print(f"ğŸ“‚ ë¡œë“œ: {section_file.name}")
    
    with open(section_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    sections = data.get('sections', [])
    if not sections:
        print("âŒ ì„¹ì…˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“„ ì„¹ì…˜ ìˆ˜: {len(sections)}ê°œ")
    
    # êµì • ì²˜ë¦¬
    print("\nğŸ”¤ í˜•íƒœì†Œ ê¸°ë°˜ êµì • ì‹œì‘...")
    corrected_sections = process_sections(sections)
    
    # ê²°ê³¼ ì €ì¥
    output_data = data.copy()
    output_data['sections'] = corrected_sections
    output_data['metadata']['pipeline_stage'] = '1ì°¨ êµì • (í˜•íƒœì†Œ)'
    output_data['metadata']['timestamp'] = datetime.now().isoformat()
    output_data['metadata']['kiwi_available'] = KIWI_AVAILABLE
    
    step5_dir = CURRENT_SESSION_DIR / "step5_kiwi"
    step5_dir.mkdir(parents=True, exist_ok=True)
    output_file = step5_dir / "kiwi_intermediate.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # í†µê³„
    changed_count = sum(1 for s in corrected_sections if s.get('text_changed', False))
    
    print(f"\nğŸ‰ 1ì°¨ êµì • ì™„ë£Œ!")
    print(f"ğŸ“„ êµì •ëœ ì„¹ì…˜: {changed_count}/{len(corrected_sections)}ê°œ")
    print(f"ğŸ“ ê²°ê³¼: {output_file}")
    print(f"\nâœ¨ ì ìš©ëœ ê¸°ëŠ¥:")
    print(f"   â€¢ ë¬¸ë§¥ ê¸°ë°˜ ìëª¨ ë³µì› (ì´ˆì„±/ì¤‘ì„±/ì¢…ì„± ì¡°í•©)")
    print(f"   â€¢ í•œê¸€+ì˜ì–´ í˜¼í•© íŒ¨í„´ ìˆ˜ì • (ì˜ˆ: ë¬´lesë¥¼ â†’ ë¬´ë¦ì„)")
    print(f"   â€¢ ë¬¸ë§¥ ê¸°ë°˜ ë‹¨ì–´ êµì • (ì˜ˆ: ë¬´ë¦„ â†’ ë¬´ë¦ ì²´ìœ¡ ë¬¸ë§¥ì—ì„œ)")
    print(f"   â€¢ Kiwi í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ ë„ì–´ì“°ê¸° êµì •")

if __name__ == "__main__":
    main()
