#!/usr/bin/env python3
"""
â‘£ ë ˆì´ì•„ì›ƒ ë³‘í•© (ì„¹ì…˜ ë‹¨ìœ„)
ì…ë ¥: ocr_refined/refined_combined.json
ì²˜ë¦¬: BeautifulSoup4ë¡œ DOM ìœ ì‚¬ êµ¬ì¡° êµ¬ì„± â†’ ì„¹ì…˜ ë³‘í•©
ì¶œë ¥: layout_combined/doc1_sections.json
"""

import json
import re
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup, Tag

# ì„¤ì •
BASE_DIR = Path(__file__).parent.parent
OCR_DIR = BASE_DIR / "ocr_results"
LAYOUT_DIR = OCR_DIR / "step4_layout"
CURRENT_SESSION_DIR = None

def html_table_to_text(html_text):
    """HTML í…Œì´ë¸”ì„ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if not html_text or '<table' not in html_text.lower():
        return html_text
    
    try:
        soup = BeautifulSoup(html_text, 'html.parser')
        tables = soup.find_all('table')
        
        if not tables:
            return html_text
        
        result_parts = []
        
        for table in tables:
            table_text = []
            
            # í…Œì´ë¸” í–‰ ì²˜ë¦¬
            rows = table.find_all('tr')
            for row in rows:
                # í—¤ë” ì…€ (th)ê³¼ ë°ì´í„° ì…€ (td) ëª¨ë‘ ì²˜ë¦¬
                cells = row.find_all(['th', 'td'])
                cell_texts = [cell.get_text().strip() for cell in cells]
                
                # ë¹ˆ ì…€ ì œê±° í›„ ë³‘í•©
                cell_texts = [c for c in cell_texts if c]
                if cell_texts:
                    row_text = ' | '.join(cell_texts)
                    table_text.append(row_text)
            
            # í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if table_text:
                result_parts.append('\n'.join(table_text))
        
        # í…Œì´ë¸” ì™¸ í…ìŠ¤íŠ¸ë„ í¬í•¨
        for elem in soup.children:
            if elem.name != 'table' and hasattr(elem, 'get_text'):
                text = elem.get_text().strip()
                if text:
                    result_parts.append(text)
        
        return '\n'.join(result_parts)
        
    except Exception as e:
        print(f"    âš ï¸ í…Œì´ë¸” ë³€í™˜ ì˜¤ë¥˜: {e}")
        return html_text

def create_dom_structure(pages_data):
    """í˜ì´ì§€ ë°ì´í„°ë¥¼ DOM êµ¬ì¡°ë¡œ ë³€í™˜ (HTML í…Œì´ë¸”ì€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)"""
    soup = BeautifulSoup('<document></document>', 'html.parser')
    doc_root = soup.document
    
    for page_data in pages_data:
        page_num = page_data.get('page_number', 0)
        page_tag = soup.new_tag('page', number=page_num)
        
        blocks = page_data.get('blocks', [])
        for i, block in enumerate(blocks):
            category = block.get('category', '').lower()
            text = block.get('text', '').strip()
            
            # Table ì¹´í…Œê³ ë¦¬ëŠ” HTMLì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if category == 'table':
                text = html_table_to_text(text)
                print(f"    ğŸ“Š Table ë³€í™˜: {len(block.get('text', ''))}ì â†’ {len(text)}ì")
            
            block_tag = soup.new_tag('block', 
                                   index=i,
                                   category=category,
                                   confidence=block.get('confidence', 0.0))
            block_tag.string = text
            page_tag.append(block_tag)
        
        doc_root.append(page_tag)
    
    return soup

def detect_section_headers(soup):
    """ì„¹ì…˜ í—¤ë” íŒ¨í„´ ê°ì§€"""
    header_patterns = [
        r'^\d+\.\s*',  # "1. ì œëª©"
        r'^[ê°€-í£]\.\s*',  # "ê°€. ì œëª©"
        r'^\(\d+\)\s*',  # "(1) ì œëª©"
        r'^[â‘ -â‘³]\s*',  # "â‘  ì œëª©"
        r'^ì œ\s*\d+\s*ì¥',  # "ì œ1ì¥"
        r'^[0-9]+\s*[ì¥ì ˆí•­]',  # "1ì¥", "2ì ˆ"
    ]
    
    sections = []
    
    for block in soup.find_all('block'):
        text = block.get_text().strip()
        category = block.get('category', '').lower()
        
        # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ íŒë‹¨
        if 'header' in category or 'title' in category:
            sections.append({
                'element': block,
                'text': text,
                'type': 'category_header',
                'level': 1
            })
            continue
        
        # íŒ¨í„´ ê¸°ë°˜ íŒë‹¨
        for i, pattern in enumerate(header_patterns):
            if re.match(pattern, text):
                level = i + 1  # íŒ¨í„´ ìˆœì„œë¡œ ë ˆë²¨ ê²°ì •
                sections.append({
                    'element': block,
                    'text': text,
                    'type': 'pattern_header',
                    'level': level
                })
                break
    
    return sections

def merge_sections(soup, section_headers):
    """ì„¹ì…˜ë³„ë¡œ ë¸”ë¡ë“¤ì„ ë³‘í•©"""
    sections = []
    current_section = None
    orphan_blocks = []  # í—¤ë” ì—†ì´ ì‹œì‘í•˜ëŠ” ë¸”ë¡ë“¤
    
    all_blocks = soup.find_all('block')
    header_blocks = {header['element'] for header in section_headers}
    
    for block in all_blocks:
        if block in header_blocks:
            # ì²« ì„¹ì…˜ ì „ì— ê³ ì•„ ë¸”ë¡ì´ ìˆìœ¼ë©´ ì„¹ì…˜ìœ¼ë¡œ ë§Œë“¤ê¸°
            if not current_section and orphan_blocks:
                orphan_section = {
                    'id': f"section_{len(sections)+1:03d}",
                    'title': '(ë¨¸ë¦¬ë§)',  # ê¸°ë³¸ ì œëª©
                    'category': 'orphan',
                    'level': 0,
                    'pages': list(set(int(b.find_parent('page')['number']) for b in orphan_blocks)),
                    'blocks': orphan_blocks.copy(),
                    'text_parts': [b.get_text().strip() for b in orphan_blocks if b.get_text().strip() and len(b.get_text().strip()) > 2]
                }
                sections.append(orphan_section)
                orphan_blocks = []
            
            # ìƒˆ ì„¹ì…˜ ì‹œì‘
            if current_section:
                sections.append(current_section)
            
            # í—¤ë” ì •ë³´ ì°¾ê¸°
            header_info = next(h for h in section_headers if h['element'] == block)
            
            current_section = {
                'id': f"section_{len(sections)+1:03d}",
                'title': header_info['text'],
                'category': block.get('category', ''),
                'level': header_info['level'],
                'pages': [int(block.find_parent('page')['number'])],
                'blocks': [block],
                'text_parts': []
            }
        else:
            # ê¸°ì¡´ ì„¹ì…˜ì— ë¸”ë¡ ì¶”ê°€
            if current_section:
                page_num = int(block.find_parent('page')['number'])
                if page_num not in current_section['pages']:
                    current_section['pages'].append(page_num)
                
                current_section['blocks'].append(block)
                
                # í…ìŠ¤íŠ¸ ì¶”ê°€ (ë¹ˆ í…ìŠ¤íŠ¸ ì œì™¸)
                text = block.get_text().strip()
                if text and len(text) > 2:
                    current_section['text_parts'].append(text)
            else:
                # ì•„ì§ ì„¹ì…˜ì´ ì‹œì‘ ì•ˆëìœ¼ë©´ ê³ ì•„ ë¸”ë¡ìœ¼ë¡œ ì €ì¥
                orphan_blocks.append(block)
    
    # ë§ˆì§€ë§‰ ì„¹ì…˜ ì¶”ê°€
    if current_section:
        sections.append(current_section)
    
    # ëì— ë‚¨ì€ ê³ ì•„ ë¸”ë¡ë“¤ë„ ì„¹ì…˜ìœ¼ë¡œ ì¶”ê°€
    if orphan_blocks:
        orphan_section = {
            'id': f"section_{len(sections)+1:03d}",
            'title': '(ì¶”ê°€ ë‚´ìš©)',
            'category': 'orphan',
            'level': 0,
            'pages': list(set(int(b.find_parent('page')['number']) for b in orphan_blocks)),
            'blocks': orphan_blocks.copy(),
            'text_parts': [b.get_text().strip() for b in orphan_blocks if b.get_text().strip() and len(b.get_text().strip()) > 2]
        }
        sections.append(orphan_section)
    
    return sections

def combine_section_text(section):
    """ì„¹ì…˜ ë‚´ í…ìŠ¤íŠ¸ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ê²°í•©"""
    text_parts = section['text_parts']
    if not text_parts:
        return ""
    
    combined = []
    
    for i, part in enumerate(text_parts):
        # ë¬¸ì¥ ë ì²˜ë¦¬
        if i > 0:
            prev_part = combined[-1]
            # ì´ì „ ë¬¸ì¥ì´ ì™„ì „í•˜ì§€ ì•Šìœ¼ë©´ ê³µë°±ìœ¼ë¡œ ì—°ê²°
            if not prev_part.endswith(('.', '!', '?', 'ë‹¤', 'ìš”', 'ë‹ˆë‹¤')):
                combined.append(' ')
        
        combined.append(part)
        
        # ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œ ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°)
        if not part.endswith(('.', '!', '?')) and i < len(text_parts) - 1:
            next_part = text_parts[i + 1]
            # ë‹¤ìŒ ë¬¸ì¥ì´ ëŒ€ë¬¸ìë‚˜ ìˆ«ìë¡œ ì‹œì‘í•˜ë©´ ë§ˆì¹¨í‘œ ì¶”ê°€
            if next_part and (next_part[0].isupper() or next_part[0].isdigit()):
                combined.append('.')
    
    return ''.join(combined).strip()

def process_refined_data():
    """ì •ì œëœ ë°ì´í„°ë¥¼ ì„¹ì…˜ë³„ë¡œ ë³‘í•©"""
    # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì½ê¸°
    session_file = BASE_DIR / "current_session.json"
    if not session_file.exists():
        print("âŒ í˜„ì¬ ì„¸ì…˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    with open(session_file, 'r', encoding='utf-8') as f:
        session_info = json.load(f)
    
    global CURRENT_SESSION_DIR
    CURRENT_SESSION_DIR = Path(session_info['session_dir'])
    
    # step1 ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ (í†µí•©ëœ íŒŒì´í”„ë¼ì¸)
    step1_dir = CURRENT_SESSION_DIR / "step1_primary"
    combined_file = step1_dir / "combined.json"
    
    if not combined_file.exists():
        print(f"âŒ í†µí•© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {combined_file}")
        return None
    
    print(f"ğŸ“‚ ì‚¬ìš©í•  íŒŒì¼: {combined_file.name}")
    
    with open(combined_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    pages_data = data.get('pages', [])
    if not pages_data:
        print("âŒ í˜ì´ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"ğŸ“„ ë¡œë“œëœ í˜ì´ì§€: {len(pages_data)}ê°œ")
    
    # DOM êµ¬ì¡° ìƒì„±
    print("ğŸ—ï¸ DOM êµ¬ì¡° ìƒì„±...")
    soup = create_dom_structure(pages_data)
    
    # ì„¹ì…˜ í—¤ë” ê°ì§€
    print("ğŸ” ì„¹ì…˜ í—¤ë” ê°ì§€...")
    section_headers = detect_section_headers(soup)
    print(f"   ë°œê²¬ëœ í—¤ë”: {len(section_headers)}ê°œ")
    
    # ì„¹ì…˜ ë³‘í•©
    print("ğŸ”— ì„¹ì…˜ ë³‘í•©...")
    sections = merge_sections(soup, section_headers)
    
    # í…ìŠ¤íŠ¸ ê²°í•©
    print("ğŸ“ í…ìŠ¤íŠ¸ ê²°í•©...")
    for section in sections:
        section['text_combined'] = combine_section_text(section)
        # DOM ìš”ì†Œ ì œê±° (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
        del section['blocks']
        del section['text_parts']
    
    print(f"âœ… {len(sections)}ê°œ ì„¹ì…˜ ìƒì„±")
    
    return {
        'document_id': data.get('document_id', 'unknown'),
        'sections': sections,
        'metadata': {
            'pipeline_stage': 'ì„¹ì…˜ ë³‘í•©',
            'timestamp': datetime.now().isoformat(),
            'total_sections': len(sections),
            'source_pages': len(pages_data)
        }
    }

def main():
    print("=" * 60)
    print("ğŸ”— ë ˆì´ì•„ì›ƒ ë³‘í•© (ì„¹ì…˜ ë‹¨ìœ„)")
    print("=" * 60)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    LAYOUT_DIR.mkdir(exist_ok=True)
    
    # ë°ì´í„° ì²˜ë¦¬
    result = process_refined_data()
    if not result:
        return
    
    # ê²°ê³¼ ì €ì¥ (ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
    step4_dir = CURRENT_SESSION_DIR / "step4_layout"
    step4_dir.mkdir(parents=True, exist_ok=True)
    
    doc_id = result['document_id']
    output_file = step4_dir / f"{doc_id}_sections.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ ì„¹ì…˜ ë³‘í•© ì™„ë£Œ!")
    print(f"ğŸ“„ ì´ ì„¹ì…˜: {len(result['sections'])}ê°œ")
    print(f"ğŸ“ ê²°ê³¼: {output_file}")
    
    # ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°
    print(f"\nğŸ“‹ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°:")
    for i, section in enumerate(result['sections'][:5]):  # ì²˜ìŒ 5ê°œë§Œ
        title = section['title'][:50] + ('...' if len(section['title']) > 50 else '')
        text_preview = section['text_combined'][:100] + ('...' if len(section['text_combined']) > 100 else '')
        print(f"   {i+1}. {title}")
        print(f"      {text_preview}")

if __name__ == "__main__":
    main()
