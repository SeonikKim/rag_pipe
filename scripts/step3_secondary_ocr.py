#!/usr/bin/env python3
"""
â‘¢ 2ì°¨ OCR (ì „ì²˜ë¦¬ ê³ ì • ì˜µì…˜ ì ìš©)
ì…ë ¥: crops/low_conf/*.png
ì „ì²˜ë¦¬: Gaussian 5Ã—5 â†’ Unsharp r=1.0~1.5, p=120~200 â†’ ì—…ìŠ¤ì¼€ì¼ Ã—2.8
ì²˜ë¦¬: ì „ì²˜ë¦¬ ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ OCR â†’ ì›ë˜ ë¸”ë¡ì— ê°’ ì¹˜í™˜
ì¶œë ¥: ocr_refined/page_XXX_refined.json, ocr_refined/refined_combined.json
"""

import os
import json
import cv2
import numpy as np
import requests
import base64
from pathlib import Path
from datetime import datetime
import glob

# ì„¤ì •
BASE_DIR = Path(__file__).parent.parent
OCR_DIR = BASE_DIR / "ocr_results"
# ì„¸ì…˜ë³„ ë””ë ‰í† ë¦¬ëŠ” ëŸ°íƒ€ì„ì— ì„¤ì •
CURRENT_SESSION_DIR = None
DOTSOCR_URL = "http://localhost:8000/v1/chat/completions"

# ì „ì²˜ë¦¬ ê³ ì • ì˜µì…˜
GAUSSIAN_KERNEL = (5, 5)
UNSHARP_RADIUS = 1.3
UNSHARP_PERCENT = 150
UPSCALE_FACTOR = 2.8

def preprocess_image_fixed(image):
    """ê³ ì • ì „ì²˜ë¦¬ ì˜µì…˜ ì ìš©"""
    # 1. Gaussian Blur (5Ã—5)
    blurred = cv2.GaussianBlur(image, GAUSSIAN_KERNEL, 0)
    
    # 2. Unsharp Mask (r=1.3, p=150)
    unsharp = cv2.addWeighted(image, 1 + UNSHARP_PERCENT/100, blurred, -UNSHARP_PERCENT/100, 0)
    
    # 3. ì—…ìŠ¤ì¼€ì¼ (Ã—2.8)
    height, width = unsharp.shape[:2]
    new_size = (int(width * UPSCALE_FACTOR), int(height * UPSCALE_FACTOR))
    upscaled = cv2.resize(unsharp, new_size, interpolation=cv2.INTER_CUBIC)
    
    return upscaled

def ocr_with_dotsocr(image, confidence_threshold=0.8):
    """DotsOCRë¡œ 2ì°¨ OCR ìˆ˜í–‰"""
    try:
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        _, buffer = cv2.imencode('.png', image)
        image_data = base64.b64encode(buffer).decode('utf-8')
        
        response = requests.post(DOTSOCR_URL, json={
            "model": "model",
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_data}"}},
                    {"type": "text", "text": """Please output the layout information from the PDF image, including each layout element's bbox, its category, and the corresponding text content within the bbox.

1. Bbox format: [x1, y1, x2, y2]

2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].

3. Text Extraction & Formatting Rules:
    - Picture: For the 'Picture' category, the text field should be omitted.
    - Formula: Format its text as LaTeX.
    - Table: Format its text as HTML.
    - All Others (Text, Title, etc.): Format their text as Markdown.

4. Constraints:
    - The output text must be the original text from the image, with no translation.
    - All layout elements must be sorted according to human reading order.

5. Final Output: The entire output must be a single JSON object."""}
                ]
            }],
            "max_tokens": 4096,
            "temperature": 0.0
        }, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content']
        else:
            return None
            
    except Exception as e:
        print(f"âŒ OCR ì‹¤íŒ¨: {e}")
        return None

def parse_crop_filename(filename):
    """í¬ë¡­ íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
    # ì˜ˆ: doc1_page001_block005.png
    parts = filename.stem.split('_')
    if len(parts) >= 3:
        doc_name = parts[0]
        page_str = parts[1].replace('page', '')
        block_str = parts[2].replace('block', '')
        
        try:
            page_num = int(page_str)
            block_idx = int(block_str)
            return doc_name, page_num, block_idx
        except ValueError:
            pass
    
    return None, None, None

def load_original_ocr_results():
    """1ì°¨ OCR ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë¡œë“œ"""
    results = {}
    ocr_files = list(OCR_DIR.glob("page_*.json"))
    
    for ocr_file in ocr_files:
        with open(ocr_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            page_num = data['page_number']
            results[page_num] = data
    
    return results

def process_crops():
    """í¬ë¡­ëœ ì´ë¯¸ì§€ë“¤ì„ 2ì°¨ OCR ì²˜ë¦¬"""
    # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì½ê¸°
    session_file = BASE_DIR / "current_session.json"
    if not session_file.exists():
        print("âŒ í˜„ì¬ ì„¸ì…˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    with open(session_file, 'r', encoding='utf-8') as f:
        session_info = json.load(f)
    
    global CURRENT_SESSION_DIR
    CURRENT_SESSION_DIR = Path(session_info['session_dir'])
    
    # í¬ë¡­ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸°
    crops_dir = CURRENT_SESSION_DIR / "step2_crops" / "low_conf"
    crop_files = list(crops_dir.glob("*.png"))
    if not crop_files:
        print("âŒ í¬ë¡­ëœ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    print(f"ğŸ“¦ í¬ë¡­ëœ ì´ë¯¸ì§€: {len(crop_files)}ê°œ")
    
    # 1ì°¨ OCR ê²°ê³¼ ë¡œë“œ
    original_results = load_original_ocr_results()
    
    # 2ì°¨ OCR ê²°ê³¼ ì €ì¥ìš©
    refined_results = {}
    
    for i, crop_file in enumerate(crop_files, 1):
        print(f"[{i}/{len(crop_files)}] {crop_file.name}")
        
        # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
        doc_name, page_num, block_idx = parse_crop_filename(crop_file)
        if not all([doc_name, page_num is not None, block_idx is not None]):
            print(f"    âš ï¸ íŒŒì¼ëª… íŒŒì‹± ì‹¤íŒ¨")
            continue
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        meta_file = crop_file.with_suffix('.json')
        if meta_file.exists():
            with open(meta_file, 'r', encoding='utf-8') as f:
                meta_data = json.load(f)
        else:
            meta_data = {}
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(str(crop_file))
        if image is None:
            print(f"    âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
            continue
        
        # ì „ì²˜ë¦¬ ì ìš©
        preprocessed = preprocess_image_fixed(image)
        
        # 2ì°¨ OCR ìˆ˜í–‰
        ocr_result = ocr_with_dotsocr(preprocessed, confidence_threshold=0.8)
        
        if ocr_result:
            # ê²°ê³¼ ì €ì¥
            if page_num not in refined_results:
                # ì›ë³¸ í˜ì´ì§€ ë°ì´í„° ë³µì‚¬
                if page_num in original_results:
                    refined_results[page_num] = original_results[page_num].copy()
                    refined_results[page_num]['blocks'] = [block.copy() for block in original_results[page_num]['blocks']]
                else:
                    refined_results[page_num] = {
                        "page_number": page_num,
                        "blocks": []
                    }
            
            # í•´ë‹¹ ë¸”ë¡ì˜ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            if block_idx < len(refined_results[page_num]['blocks']):
                old_text = refined_results[page_num]['blocks'][block_idx].get('text', '')
                refined_results[page_num]['blocks'][block_idx]['text'] = ocr_result
                refined_results[page_num]['blocks'][block_idx]['ocr_method'] = '2nd_pass_fixed_preprocess'
                refined_results[page_num]['blocks'][block_idx]['preprocessing'] = {
                    "gaussian": f"{GAUSSIAN_KERNEL}",
                    "unsharp": f"r={UNSHARP_RADIUS}, p={UNSHARP_PERCENT}",
                    "upscale": f"Ã—{UPSCALE_FACTOR}"
                }
                
                print(f"    âœ… {len(old_text)} â†’ {len(ocr_result)}ì")
            else:
                print(f"    âš ï¸ ë¸”ë¡ ì¸ë±ìŠ¤ ì˜¤ë¥˜: {block_idx}")
        else:
            print(f"    âŒ OCR ì‹¤íŒ¨")
    
    return refined_results

def save_refined_results(refined_results):
    """ì •ì œëœ ê²°ê³¼ ì €ì¥"""
    REFINED_DIR.mkdir(exist_ok=True)
    
    # í˜ì´ì§€ë³„ ì €ì¥
    for page_num, page_data in refined_results.items():
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        page_data['pipeline_stage'] = '2ì°¨ OCR (ê³ ì • ì „ì²˜ë¦¬)'
        page_data['timestamp'] = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # í˜ì´ì§€ë³„ íŒŒì¼ ì €ì¥ (ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
        step3_dir = CURRENT_SESSION_DIR / "step3_refined"
        step3_dir.mkdir(parents=True, exist_ok=True)
        output_path = step3_dir / f"page_{page_num:03d}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(page_data, f, ensure_ascii=False, indent=2)
    
    # í†µí•© íŒŒì¼ ì €ì¥
    combined_data = {
        "timestamp": session_info['session_timestamp'],
        "pipeline_stage": "2ì°¨ OCR í†µí•©",
        "preprocessing": {
            "gaussian": f"{GAUSSIAN_KERNEL}",
            "unsharp": f"r={UNSHARP_RADIUS}, p={UNSHARP_PERCENT}",
            "upscale": f"Ã—{UPSCALE_FACTOR}"
        },
        "pages": list(refined_results.values())
    }
    
    combined_path = step3_dir / "refined_combined.json"
    with open(combined_path, 'w', encoding='utf-8') as f:
        json.dump(combined_data, f, ensure_ascii=False, indent=2)
    
    return combined_path

def main():
    print("=" * 60)
    print("ğŸ”§ 2ì°¨ OCR (ê³ ì • ì „ì²˜ë¦¬ ì˜µì…˜)")
    print("=" * 60)
    print(f"ğŸ“‹ ì „ì²˜ë¦¬ ì„¤ì •:")
    print(f"   â€¢ Gaussian: {GAUSSIAN_KERNEL}")
    print(f"   â€¢ Unsharp: r={UNSHARP_RADIUS}, p={UNSHARP_PERCENT}")
    print(f"   â€¢ ì—…ìŠ¤ì¼€ì¼: Ã—{UPSCALE_FACTOR}")
    
    # DotsOCR ì„œë²„ í™•ì¸
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        print("âœ… DotsOCR ì„œë²„ ì—°ê²°ë¨")
    except:
        print("âŒ DotsOCR ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í¬ë¡­ ì´ë¯¸ì§€ ì²˜ë¦¬
    print("\nğŸ” í¬ë¡­ ì´ë¯¸ì§€ 2ì°¨ OCR ì²˜ë¦¬...")
    refined_results = process_crops()
    
    if not refined_results:
        print("âŒ ì²˜ë¦¬í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥...")
    combined_path = save_refined_results(refined_results)
    
    print(f"\nğŸ‰ 2ì°¨ OCR ì™„ë£Œ!")
    print(f"ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: {len(refined_results)}ê°œ")
    print(f"ğŸ“ ê²°ê³¼: {REFINED_DIR}")
    print(f"ğŸ“„ í†µí•© íŒŒì¼: {combined_path}")

if __name__ == "__main__":
    main()
