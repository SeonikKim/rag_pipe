#!/usr/bin/env python3
"""
â‘¡ ì €ì‹ ë¢° ë¸”ë¡ ì„ ë³„Â·í¬ë¡­
ì…ë ¥: 1ì°¨ OCR JSON
ì²˜ë¦¬: confidence < ì„ê³„ì¹˜ ë¸”ë¡ë§Œ ì¢Œí‘œ í¬ë¡­
ì¶œë ¥: crops/low_conf/*.png, crops/images/*.png
"""

import os
import json
import cv2
import numpy as np
from pathlib import Path
import glob

# ì„¤ì •
BASE_DIR = Path(__file__).parent.parent
OCR_DIR = BASE_DIR / "ocr_results"
DEBUG_DIR = OCR_DIR / "debug"
# ì„¸ì…˜ë³„ ë””ë ‰í† ë¦¬ëŠ” ëŸ°íƒ€ì„ì— ì„¤ì •
CURRENT_SESSION_DIR = None

CONFIDENCE_THRESHOLD = 0.95  # ë‹¨ì–´ë³„ confidence ì„ê³„ì¹˜
BLOCK_CONFIDENCE_THRESHOLD = 0.80  # ë¸”ë¡ ì „ì²´ confidence ì„ê³„ì¹˜
MARGIN = 15  # í¬ë¡­ ì‹œ ì—¬ë°±
WORD_MARGIN = 5  # ë‹¨ì–´ í¬ë¡­ ì‹œ ì—¬ë°±

def load_ocr_results():
    """1ì°¨ OCR ê²°ê³¼ ë¡œë“œ (í˜„ì¬ ì„¸ì…˜)"""
    # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì½ê¸°
    session_file = BASE_DIR / "current_session.json"
    if not session_file.exists():
        print("âŒ í˜„ì¬ ì„¸ì…˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. step1ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return []
    
    with open(session_file, 'r', encoding='utf-8') as f:
        session_info = json.load(f)
    
    session_dir = Path(session_info['session_dir'])
    step1_dir = session_dir / "step1_primary"
    
    if not step1_dir.exists():
        return []
    
    # í˜ì´ì§€ë³„ JSON íŒŒì¼ ë¡œë“œ
    results = []
    page_files = sorted(step1_dir.glob("page_*.json"))
    
    for page_file in page_files:
        with open(page_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            results.append(data)
    
    # ê¸€ë¡œë²Œ ì„¸ì…˜ ì •ë³´ ì €ì¥ (ë‹¤ë¥¸ ë‹¨ê³„ì—ì„œ ì‚¬ìš©)
    global CURRENT_SESSION_DIR
    CURRENT_SESSION_DIR = session_dir
    
    return results

def is_low_confidence_block(block, threshold=BLOCK_CONFIDENCE_THRESHOLD):
    """ì €ì‹ ë¢° ë¸”ë¡ íŒë‹¨ (ë¸”ë¡ ì „ì²´ ê¸°ì¤€)"""
    confidence = block.get('confidence', 0.0)
    
    # confidenceê°€ ì„ê³„ì¹˜ ë¯¸ë§Œ
    if confidence < threshold:
        return True
    
    # ì¶”ê°€ ì¡°ê±´ë“¤
    text = block.get('text', '').strip()
    
    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ
    if len(text) < 3:
        return True
    
    # í•œê¸€ ë¹„ìœ¨ì´ ë‚®ìŒ
    korean_chars = sum(1 for c in text if '\uac00' <= c <= '\ud7af')
    if len(text) > 0 and korean_chars / len(text) < 0.6:
        return True
    
    # íŠ¹ìˆ˜ ë¬¸ìê°€ ë§ìŒ
    special_chars = sum(1 for c in text if not c.isalnum() and c not in ' \n\t')
    if len(text) > 0 and special_chars / len(text) > 0.3:
        return True
    
    return False

def has_low_confidence_words(block):
    """ì €ì‹ ë¢° ë‹¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸"""
    low_conf_tokens = block.get('low_confidence_tokens', [])
    if not low_conf_tokens:
        return False
    
    # confidence < 0.95ì¸ ë‹¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸
    for token_info in low_conf_tokens:
        confidence = token_info.get('confidence', 0.0)
        if confidence < CONFIDENCE_THRESHOLD:
            return True
    
    return False

def crop_block(image, bbox, margin=MARGIN):
    """ë¸”ë¡ í¬ë¡­ (ì—¬ë°± í¬í•¨)"""
    h, w = image.shape[:2]
    
    if len(bbox) == 4:
        x, y, width, height = bbox
        x2, y2 = x + width, y + height
    elif len(bbox) == 8:  # 4ì  ì¢Œí‘œ
        xs = [bbox[i] for i in range(0, 8, 2)]
        ys = [bbox[i] for i in range(1, 8, 2)]
        x, y = min(xs), min(ys)
        x2, y2 = max(xs), max(ys)
    else:
        return None
    
    # ì—¬ë°± ì¶”ê°€
    x1 = max(0, int(x - margin))
    y1 = max(0, int(y - margin))
    x2 = min(w, int(x2 + margin))
    y2 = min(h, int(y2 + margin))
    
    if x2 <= x1 or y2 <= y1:
        return None
    
    return image[y1:y2, x1:x2]

def estimate_word_bbox(block_bbox, text, word_index, total_words):
    """ë‹¨ì–´ì˜ ëŒ€ëµì ì¸ bbox ì¶”ì •"""
    x1, y1, x2, y2 = block_bbox
    
    # ë¸”ë¡ì˜ ë„ˆë¹„ë¥¼ ë‹¨ì–´ ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ê° ë‹¨ì–´ì˜ ë„ˆë¹„ ì¶”ì •
    block_width = x2 - x1
    word_width = block_width / max(total_words, 1)
    
    # ë‹¨ì–´ì˜ x ì¢Œí‘œ ê³„ì‚°
    word_x1 = x1 + (word_index * word_width)
    word_x2 = min(x2, word_x1 + word_width)
    
    return [int(word_x1), y1, int(word_x2), y2]

def crop_low_confidence_words(image, block, margin=WORD_MARGIN):
    """ì €ì‹ ë¢° ë‹¨ì–´ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ í¬ë¡­"""
    low_conf_tokens = block.get('low_confidence_tokens', [])
    if not low_conf_tokens:
        return []
    
    block_bbox = block.get('bbox', [0, 0, 100, 100])
    if len(block_bbox) != 4:
        return []
    
    cropped_words = []
    
    for i, token_info in enumerate(low_conf_tokens):
        token = token_info.get('token', '')
        confidence = token_info.get('confidence', 0.0)
        
        # confidenceê°€ ì„ê³„ì¹˜ ë¯¸ë§Œì¸ ê²½ìš°ë§Œ í¬ë¡­
        if confidence < CONFIDENCE_THRESHOLD:
            # ë‹¨ì–´ì˜ ëŒ€ëµì ì¸ bbox ì¶”ì •
            word_bbox = estimate_word_bbox(block_bbox, block.get('text', ''), i, len(low_conf_tokens))
            
            # ë‹¨ì–´ í¬ë¡­
            cropped_word = crop_block(image, word_bbox, margin)
            if cropped_word is not None and cropped_word.size > 0:
                cropped_words.append({
                    'cropped_image': cropped_word,
                    'token': token,
                    'confidence': confidence,
                    'word_bbox': word_bbox,
                    'original_bbox': block_bbox
                })
    
    return cropped_words

def categorize_block(block):
    """ë¸”ë¡ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    category = block.get('category', '').lower()
    text = block.get('text', '').strip()
    
    # ë¹„í…ìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬
    non_text_categories = ['figure', 'table', 'image', 'chart', 'graph']
    if any(cat in category for cat in non_text_categories):
        return 'images'
    
    # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš°
    if len(text) < 5:
        return 'images'
    
    return 'low_conf'

def process_page(page_data):
    """í˜ì´ì§€ë³„ ì €ì‹ ë¢° ë¸”ë¡ ì²˜ë¦¬"""
    page_num = page_data['page_number']
    doc_name = page_data['document_name']
    
    # ë””ë²„ê·¸ ì´ë¯¸ì§€ ë¡œë“œ
    debug_image_path = DEBUG_DIR / f"page_{page_num:03d}.png"
    if not debug_image_path.exists():
        print(f"âŒ ë””ë²„ê·¸ ì´ë¯¸ì§€ ì—†ìŒ: {debug_image_path}")
        return 0
    
    image = cv2.imread(str(debug_image_path))
    if image is None:
        print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {debug_image_path}")
        return 0
    
    crop_count = 0
    blocks = page_data.get('blocks', [])
    
    # ì„¸ì…˜ë³„ í¬ë¡­ ë””ë ‰í† ë¦¬ ì„¤ì •
    step2_dir = CURRENT_SESSION_DIR / "step2_crops"
    low_conf_dir = step2_dir / "low_conf"
    words_dir = step2_dir / "words"
    images_dir = step2_dir / "images"
    
    for i, block in enumerate(blocks):
        if 'bbox' not in block:
            continue
        
        category = block.get('category', '').lower()
        
        # 1. í…ìŠ¤íŠ¸ ë¸”ë¡ì—ì„œ ì €ì‹ ë¢° ë‹¨ì–´ í¬ë¡­
        if category in ['text', 'title', 'section-header'] and has_low_confidence_words(block):
            cropped_words = crop_low_confidence_words(image, block)
            
            for j, word_data in enumerate(cropped_words):
                words_dir.mkdir(parents=True, exist_ok=True)
                
                # íŒŒì¼ëª… ìƒì„±
                filename = f"{doc_name}_page{page_num:03d}_block{i:03d}_word{j:03d}.png"
                output_path = words_dir / filename
                
                # ì €ì¥
                cv2.imwrite(str(output_path), word_data['cropped_image'])
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                meta_path = output_path.with_suffix('.json')
                meta_data = {
                    "source_page": page_num,
                    "block_index": i,
                    "word_index": j,
                    "token": word_data['token'],
                    "confidence": word_data['confidence'],
                    "word_bbox": word_data['word_bbox'],
                    "original_bbox": word_data['original_bbox'],
                    "category": block.get('category', ''),
                    "text_preview": block.get('text', '')[:100],
                    "reason": "low_confidence_word"
                }
                
                with open(meta_path, 'w', encoding='utf-8') as f:
                    json.dump(meta_data, f, ensure_ascii=False, indent=2)
                
                crop_count += 1
                print(f"    ğŸ”¤ Block {i:03d} Word {j:03d}: '{word_data['token']}' ({word_data['confidence']:.3f})")
        
        # 2. ì €ì‹ ë¢° ë¸”ë¡ ì „ì²´ í¬ë¡­ (ê¸°ì¡´ ë¡œì§)
        elif is_low_confidence_block(block):
            cropped = crop_block(image, block['bbox'])
            if cropped is None or cropped.size == 0:
                continue
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì €ì¥
            block_category = categorize_block(block)
            if block_category == 'low_conf':
                output_dir = low_conf_dir
            else:
                output_dir = images_dir
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„±
            filename = f"{doc_name}_page{page_num:03d}_block{i:03d}.png"
            output_path = output_dir / filename
            
            # ì €ì¥
            cv2.imwrite(str(output_path), cropped)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            meta_path = output_path.with_suffix('.json')
            meta_data = {
                "source_page": page_num,
                "block_index": i,
                "original_bbox": block['bbox'],
                "confidence": block.get('confidence', 0.0),
                "category": block.get('category', ''),
                "text_preview": block.get('text', '')[:100],
                "reason": "low_confidence_block" if block_category == 'low_conf' else "non_text"
            }
            
            with open(meta_path, 'w', encoding='utf-8') as f:
                json.dump(meta_data, f, ensure_ascii=False, indent=2)
            
            crop_count += 1
            print(f"    ğŸ“¦ Block {i:03d}: {block_category} ({block.get('confidence', 0.0):.2f})")
    
    return crop_count

def main():
    print("=" * 60)
    print("ğŸ” ì €ì‹ ë¢° ë¸”ë¡ ë° ë‹¨ì–´ ì„ ë³„ ë° í¬ë¡­")
    print("=" * 60)
    
    # 1ì°¨ OCR ê²°ê³¼ ë¡œë“œ
    print("ğŸ“‚ 1ì°¨ OCR ê²°ê³¼ ë¡œë“œ...")
    ocr_results = load_ocr_results()
    
    if not ocr_results:
        print("âŒ 1ì°¨ OCR ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… {len(ocr_results)}í˜ì´ì§€ ë¡œë“œë¨")
    print(f"ğŸ¯ ë‹¨ì–´ë³„ confidence ì„ê³„ì¹˜: {CONFIDENCE_THRESHOLD}")
    print(f"ğŸ¯ ë¸”ë¡ë³„ confidence ì„ê³„ì¹˜: {BLOCK_CONFIDENCE_THRESHOLD}")
    
    # í˜ì´ì§€ë³„ ì²˜ë¦¬
    total_crops = 0
    for page_data in ocr_results:
        page_num = page_data['page_number']
        print(f"\nğŸ“„ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘...")
        
        crops = process_page(page_data)
        total_crops += crops
        
        if crops > 0:
            print(f"    âœ… {crops}ê°œ í•­ëª© í¬ë¡­ë¨")
        else:
            print(f"    â„¹ï¸ ì €ì‹ ë¢° í•­ëª© ì—†ìŒ")
    
    print(f"\nğŸ‰ í¬ë¡­ ì™„ë£Œ!")
    print(f"ğŸ“¦ ì´ {total_crops}ê°œ í•­ëª© í¬ë¡­ë¨")
    if CURRENT_SESSION_DIR:
        step2_dir = CURRENT_SESSION_DIR / "step2_crops"
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {step2_dir}")
        print(f"  â”œâ”€â”€ words/ (ì €ì‹ ë¢° ë‹¨ì–´)")
        print(f"  â”œâ”€â”€ low_conf/ (ì €ì‹ ë¢° ë¸”ë¡)")
        print(f"  â””â”€â”€ images/ (ì´ë¯¸ì§€ ë¸”ë¡)")

if __name__ == "__main__":
    main()
