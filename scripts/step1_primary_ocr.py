#!/usr/bin/env python3
"""
â‘  ë ˆì´ì•„ì›ƒ ê°ì§€ + ì›ë³¸ OCR
ì…ë ¥: pdf_in/*.pdf
ì²˜ë¦¬: DotsOCRë¡œ ë ˆì´ì•„ì›ƒ(bbox) + ì›ë³¸ OCR ì¶”ì¶œ â†’ PictureëŠ” ì›ë³¸ì—ì„œ í¬ë¡­
ì¶œë ¥: step1_primary/page_XXX.json (ocr_raw í¬í•¨), crops/*.png (ì›ë³¸ ì´ë¯¸ì§€ í¬ë¡­)
Note: step1bì—ì„œ ì „ì²˜ë¦¬ OCR(ocr_preprocessed)ì„ ì¶”ê°€ë¡œ ìˆ˜í–‰
"""

import os
import json
import requests
import base64
import cv2
import numpy as np
import time
from datetime import datetime
import fitz  # PyMuPDF
import sys
from pathlib import Path
from PIL import Image, ImageEnhance, ImageFilter
import torch
from sentence_transformers import SentenceTransformer
from difflib import SequenceMatcher
import re

# ì„¤ì •
DOTSOCR_URL = "http://localhost:8000/v1/chat/completions"
BASE_DIR = Path(__file__).parent.parent
PDF_DIR = BASE_DIR / "pdf_in"
OCR_DIR = BASE_DIR / "ocr_results"
DEBUG_DIR = OCR_DIR / "debug"

def pdf_to_images(pdf_path, dpi=300):
    """PDFë¥¼ í˜ì´ì§€ë³„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    doc = fitz.open(pdf_path)
    images = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        mat = fitz.Matrix(dpi/72, dpi/72)  # 1ì°¨ OCRìš© ê¸°ë³¸ DPI
        pix = page.get_pixmap(matrix=mat)
        img_data = pix.tobytes("png")
        
        # OpenCVë¡œ ë³€í™˜
        nparr = np.frombuffer(img_data, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        images.append((page_num + 1, img))
        
        # ë””ë²„ê·¸ ì´ë¯¸ì§€ ì €ì¥ (ì„ íƒì‚¬í•­)
        # debug_path = DEBUG_DIR / f"page_{page_num+1:03d}.png"
        # cv2.imwrite(str(debug_path), img)
    
    doc.close()
    return images

def load_embedding_models(load_clip=False):
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    try:
        # BGE-m3-ko í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸
        bge_model = SentenceTransformer('dragonkue/bge-m3-ko')
        print("âœ… BGE-m3-ko ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # CLIP ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸) - ì„ íƒì 
        clip_model = None
        if load_clip:
            try:
                clip_model = SentenceTransformer('clip-ViT-B-32')
                print("âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                print("âš ï¸ CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ì´ë¯¸ì§€ ì„ë² ë”© ë¹„í™œì„±í™”")
        else:
            print("â­ï¸ CLIP ëª¨ë¸ ê±´ë„ˆëœ€ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
        
        return bge_model, clip_model
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def generate_text_embedding(text, bge_model):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    if bge_model is None or not text:
        return None
    
    try:
        embedding = bge_model.encode(text)
        return embedding.tolist()  # numpy arrayë¥¼ listë¡œ ë³€í™˜
    except Exception as e:
        print(f"    âš ï¸ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def generate_image_embedding(image_path, clip_model):
    """ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±"""
    if clip_model is None or not os.path.exists(image_path):
        return None
    
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(image_path).convert('RGB')
        
        # CLIPìœ¼ë¡œ ì´ë¯¸ì§€ ì„ë² ë”©
        embedding = clip_model.encode(image)
        return embedding.tolist()  # numpy arrayë¥¼ listë¡œ ë³€í™˜
    except Exception as e:
        print(f"    âš ï¸ ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def generate_image_caption(image_path):
    """ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ë°©ë²•)"""
    try:
        # ì´ë¯¸ì§€ ë¶„ì„ì„ í†µí•œ ê°„ë‹¨í•œ ìº¡ì…˜ ìƒì„±
        image = cv2.imread(image_path)
        if image is None:
            return "ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        height, width = image.shape[:2]
        
        # ì´ë¯¸ì§€ í¬ê¸° ê¸°ë°˜ ì„¤ëª…
        size_desc = f"{width}x{height} í”½ì…€ ì´ë¯¸ì§€"
        
        # ìƒ‰ìƒ ë¶„ì„
        mean_color = np.mean(image, axis=(0, 1))
        if np.mean(mean_color) > 200:
            color_desc = "ë°ì€ ì´ë¯¸ì§€"
        elif np.mean(mean_color) < 50:
            color_desc = "ì–´ë‘ìš´ ì´ë¯¸ì§€"
        else:
            color_desc = "ì¤‘ê°„ í†¤ ì´ë¯¸ì§€"
        
        # ê°„ë‹¨í•œ ìº¡ì…˜ ìƒì„±
        caption = f"ë¬¸ì„œ ë‚´ {color_desc} ({size_desc})"
        
        return caption
    except Exception as e:
        print(f"    âš ï¸ ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
        return "ì´ë¯¸ì§€ ìº¡ì…˜ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

def correct_incomplete_text(blocks):
    """ë¶ˆì™„ì „í•œ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ì™€ ë¹„êµí•˜ì—¬ êµì •"""
    if len(blocks) < 2:
        return blocks
    
    # ë¶ˆì™„ì „í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ê°ì§€
    incomplete_patterns = [
        r'.*ë„\s*clo$',  # "ë³´ì¥ë„ clo"
        r'.*ì§€\s*clo$',  # "ë³´ì¥ì§€ clo" 
        r'.*ëŠ”\s*clo$',  # "ë³´ì¥ëŠ” clo"
        r'.*ì´\s*clo$',  # "ë³´ì¥ì´ clo"
        r'.*ì„\s*clo$',  # "ë³´ì¥ì„ clo"
        r'.*ë¥¼\s*clo$',  # "ë³´ì¥ë¥¼ clo"
        r'.*ì—\s*clo$',  # "ë³´ì¥ì— clo"
        r'.*ì—ì„œ\s*clo$', # "ë³´ì¥ì—ì„œ clo"
        r'^clo.*',       # "clo"ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
        r'.*clo$',       # "clo"ë¡œ ëë‚˜ëŠ” ê²½ìš°
    ]
    
    corrected_blocks = []
    
    for i, block in enumerate(blocks):
        text = block.get('text', '').strip()
        is_incomplete = any(re.search(pattern, text, re.IGNORECASE) for pattern in incomplete_patterns)
        
        if is_incomplete and len(text) > 3:  # ìµœì†Œ 3ê¸€ì ì´ìƒ
            print(f"    ğŸ” ë¶ˆì™„ì „í•œ í…ìŠ¤íŠ¸ ê°ì§€: '{text}'")
            
            # ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
            best_match = None
            best_similarity = 0.0
            
            for j, other_block in enumerate(blocks):
                if i != j:
                    other_text = other_block.get('text', '').strip()
                    if len(other_text) > len(text):  # ë” ê¸´ í…ìŠ¤íŠ¸ì™€ ë¹„êµ
                        # ìœ ì‚¬ë„ ê³„ì‚° (ê³µí†µ ë¶€ë¶„ ê¸°ë°˜)
                        similarity = SequenceMatcher(None, text, other_text).ratio()
                        
                        # ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ë„ í™•ì¸
                        if text in other_text:
                            similarity += 0.3  # ë¶€ë¶„ ë§¤ì¹­ ë³´ë„ˆìŠ¤
                        
                        if similarity > best_similarity and similarity > 0.6:
                            best_similarity = similarity
                            best_match = other_text
            
            if best_match:
                # ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¶€ë¶„ ì¶”ì¶œ
                corrected_text = extract_similar_text(text, best_match)
                if corrected_text != text:
                    print(f"    âœ… í…ìŠ¤íŠ¸ êµì •: '{text}' â†’ '{corrected_text}' (ìœ ì‚¬ë„: {best_similarity:.2f})")
                    block['text'] = corrected_text
                else:
                    print(f"    âš ï¸ êµì • ì‹¤íŒ¨: '{text}'")
            else:
                print(f"    âš ï¸ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ì—†ìŒ: '{text}'")
        
        corrected_blocks.append(block)
    
    return corrected_blocks

def extract_similar_text(incomplete_text, full_text):
    """ë¶ˆì™„ì „í•œ í…ìŠ¤íŠ¸ë¥¼ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ìœ ì‚¬í•œ ë¶€ë¶„ìœ¼ë¡œ êµì •"""
    # "ë³´ì¥ë„ clo" â†’ "ë³´ì¥ì§€ë„" ê°™ì€ íŒ¨í„´ êµì •
    if incomplete_text.endswith(' clo') or incomplete_text.endswith('clo'):
        # "clo" ë¶€ë¶„ì„ ì œê±°í•˜ê³  ìœ ì‚¬í•œ ì–´ë¯¸ ì°¾ê¸°
        base_text = incomplete_text.replace(' clo', '').replace('clo', '').strip()
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¶€ë¶„ ì°¾ê¸°
        if base_text in full_text:
            # í•´ë‹¹ ë¶€ë¶„ ì£¼ë³€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            start_idx = full_text.find(base_text)
            if start_idx >= 0:
                # ì•ë’¤ë¡œ ëª‡ ê¸€ì ë” í¬í•¨í•´ì„œ ì¶”ì¶œ
                start = max(0, start_idx - 2)
                end = min(len(full_text), start_idx + len(base_text) + 5)
                extracted = full_text[start:end]
                
                # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
                extracted = re.sub(r'[^\w\sê°€-í£]', '', extracted).strip()
                return extracted
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë¶€ë¶„ ë°˜í™˜
    return incomplete_text

def apply_preprocessing(image):
    """ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© (ì—…ìŠ¤ì¼€ì¼ë§ + íƒ€ì¼ë§ í¬í•¨)"""
    # OpenCV ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜
    if len(image.shape) == 3:
        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    else:
        pil_image = Image.fromarray(image)
    
    # 1. Gaussian Blur (5Ã—5)
    gaussian_kernel = ImageFilter.GaussianBlur(radius=2.5)  # 5Ã—5 ì»¤ë„ì— í•´ë‹¹
    blurred = pil_image.filter(gaussian_kernel)
    
    # 2. Unsharp Mask (r=1.0~1.5, p=120~200)
    unsharp_filter = ImageFilter.UnsharpMask(radius=1.2, percent=160, threshold=3)
    sharpened = blurred.filter(unsharp_filter)
    
    # 3. ì—…ìŠ¤ì¼€ì¼ë§ (2.8ë°°)
    original_size = sharpened.size
    scale_factor = 2.8
    new_size = (int(original_size[0] * scale_factor), int(original_size[1] * scale_factor))
    upscaled = sharpened.resize(new_size, Image.Resampling.LANCZOS)
    
    # PILì„ OpenCVë¡œ ë‹¤ì‹œ ë³€í™˜
    upscaled_cv = cv2.cvtColor(np.array(upscaled), cv2.COLOR_RGB2BGR)
    
    return upscaled_cv

def extract_layout_only(original_image, timeout=300):
    """
    ë ˆì´ì•„ì›ƒë§Œ ì¶”ì¶œ (í…ìŠ¤íŠ¸ OCR ì—†ìŒ)
    1. ì›ë³¸ ì´ë¯¸ì§€ë¡œ ë ˆì´ì•„ì›ƒ(bbox)ë§Œ ì¶”ì¶œ
    2. Picture/Image ë¸”ë¡ ê°ì§€
    â†’ í…ìŠ¤íŠ¸ëŠ” step1bì—ì„œ ë³„ë„ë¡œ ì²˜ë¦¬
    """
    print(f"    ğŸ“ ë ˆì´ì•„ì›ƒ ê°ì§€ ì¤‘ (í…ìŠ¤íŠ¸ ì—†ìŒ)...")
    
    # ì›ë³¸ ì´ë¯¸ì§€ë¡œ ë ˆì´ì•„ì›ƒë§Œ ì¶”ì¶œ
    h_orig, w_orig = original_image.shape[:2]
    
    print(f"       ì›ë³¸ ì´ë¯¸ì§€: {w_orig}Ã—{h_orig}")
    
    # DotsOCRë¡œ ë ˆì´ì•„ì›ƒë§Œ ì¶”ì¶œ (í…ìŠ¤íŠ¸ ì—†ìŒ)
    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 85]
    _, buffer = cv2.imencode('.jpg', original_image, encode_param)
    image_data = base64.b64encode(buffer).decode('utf-8')
    
    try:
        response = requests.post(DOTSOCR_URL, json={
            "model": "model",
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}},
                    {"type": "text", "text": """Please output the layout information from the PDF image, including each layout element's bbox, its category, and the corresponding text content within the bbox.

1. Bbox format: [x1, y1, x2, y2]

2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].

3. Text Extraction & Formatting Rules:
    - Picture: For the 'Picture' category, the text field should be omitted.
    - Formula: Format its text as LaTeX.
    - Table: Format its text as HTML.
    - All Others (Text, Title, etc.): Format their text as Markdown.

4. Constraints:
    - The output text must be the original text from the image, with no translation.
    - All layout elements must be sorted according to human reading order.

5. Final Output: The entire output must be a single JSON object."""}
                ]
            }],
            "max_tokens": 4096,
            "temperature": 0.0
        }, timeout=timeout)
        
        if response.status_code != 200:
            print(f"       âŒ ë ˆì´ì•„ì›ƒ ì¶”ì¶œ ì‹¤íŒ¨: HTTP {response.status_code}")
            return None
            
        result = response.json()
        layout_content = result['choices'][0]['message']['content']
        print(f"       âœ… ë ˆì´ì•„ì›ƒ ì¶”ì¶œ ì™„ë£Œ: {len(layout_content)}ì")
        
        # JSON íŒŒì‹±
        import json
        layout_data = json.loads(layout_content)
        
        # blocks ì¶”ì¶œ
        if isinstance(layout_data, dict):
            blocks = layout_data.get('blocks') or layout_data.get('layout') or layout_data.get('elements', [])
            if not blocks and len(layout_data) == 1:
                blocks = list(layout_data.values())[0]
        else:
            blocks = layout_data
            
        if not blocks:
            print(f"       âŒ ë ˆì´ì•„ì›ƒ ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        print(f"    ğŸ“ {len(blocks)}ê°œ ë ˆì´ì•„ì›ƒ ë¸”ë¡ ê°ì§€ (ì›ë³¸ OCR í¬í•¨)")
        
        # ë ˆì´ì•„ì›ƒ + ì›ë³¸ OCR í…ìŠ¤íŠ¸ ë°˜í™˜
        for i, block in enumerate(blocks):
            category = block.get('category', 'Unknown')
            bbox = block.get('bbox', [])
            text = block.get('text', '')
            
            if len(bbox) != 4:
                continue
            
            # ì›ë³¸ OCR í…ìŠ¤íŠ¸ ì €ì¥
            block['ocr_raw'] = text  # step1ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ = ì›ë³¸ OCR
            block['ocr_pending'] = True  # step1bì—ì„œ ì „ì²˜ë¦¬ OCR ì¶”ê°€ í•„ìš”
            
            if category.lower() in ['picture', 'image']:
                print(f"       [{i+1:03d}] {category:15s} - Picture (ì›ë³¸ í¬ë¡­ ì˜ˆì •)")
            else:
                text_preview = text[:30] if text else '(í…ìŠ¤íŠ¸ ì—†ìŒ)'
                print(f"       [{i+1:03d}] {category:15s} - ì›ë³¸ OCR: '{text_preview}...'")
        
        print(f"    âœ… ë ˆì´ì•„ì›ƒ + ì›ë³¸ OCR ì™„ë£Œ: {len(blocks)}ê°œ ë¸”ë¡")
        return {"blocks": blocks}
        
    except Exception as e:
        print(f"       âŒ ë ˆì´ì•„ì›ƒ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def ocr_tile_direct(tile_image, timeout=20):
    """íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬ ì—†ì´ ì§ì ‘ OCR (ë¬´í•œì¬ê·€ ë°©ì§€)"""
    try:
        # ì´ë¯¸ì§€ ì¸ì½”ë”© (ì „ì²˜ë¦¬ ì—†ì´!)
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 85]
        _, buffer = cv2.imencode('.jpg', tile_image, encode_param)
        image_data = base64.b64encode(buffer).decode('utf-8')
        
        # DotsOCR ìš”ì²­
        response = requests.post(DOTSOCR_URL, json={
            "model": "model",
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}},
                    {"type": "text", "text": """Please output the layout information from the PDF image, including each layout element's bbox, its category, and the corresponding text content within the bbox.

1. Bbox format: [x1, y1, x2, y2]

2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].

3. Text Extraction & Formatting Rules:
    - Picture: For the 'Picture' category, the text field should be omitted.
    - Formula: Format its text as LaTeX.
    - Table: Format its text as HTML.
    - All Others (Text, Title, etc.): Format their text as Markdown.

4. Constraints:
    - The output text must be the original text from the image, with no translation.
    - All layout elements must be sorted according to human reading order.

5. Final Output: The entire output must be a single JSON object."""}
                ]
            }],
            "max_tokens": 4096,
            "temperature": 0.0
        }, timeout=timeout)
        
        if response.status_code == 200:
            result = response.json()
            ocr_content = result['choices'][0]['message']['content']
            
            # JSON íŒŒì‹±
            import json
            ocr_data = json.loads(ocr_content)
            
            # blocks ì¶”ì¶œ
            if isinstance(ocr_data, dict):
                blocks = ocr_data.get('blocks') or ocr_data.get('layout') or ocr_data.get('elements', [])
                if not blocks and len(ocr_data) == 1:
                    blocks = list(ocr_data.values())[0]
            else:
                blocks = ocr_data
                
            if isinstance(blocks, list) and blocks:
                return {"blocks": blocks}
            else:
                return {"blocks": []}
        else:
            return None
            
    except Exception as e:
        print(f"        âš ï¸ íƒ€ì¼ OCR ì˜¤ë¥˜: {e}")
        return None

def apply_tiling_ocr(image, tile_size=(1024, 1024), overlap=100):
    """ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë‚˜ëˆ„ì–´ OCR ìˆ˜í–‰ í›„ ê²°ê³¼ ë³‘í•©"""
    h, w = image.shape[:2]
    
    # íƒ€ì¼ í¬ê¸° ì¡°ì • (ì´ë¯¸ì§€ë³´ë‹¤ í´ ê²½ìš° ì „ì²´ ì´ë¯¸ì§€ ì‚¬ìš©)
    tile_h, tile_w = min(tile_size[0], h), min(tile_size[1], w)
    
    # ì˜¤ë²„ë©ì„ ê³ ë ¤í•œ ìŠ¤í… í¬ê¸°
    step_h = tile_h - overlap
    step_w = tile_w - overlap
    
    all_blocks = []
    
    print(f"    ğŸ§© íƒ€ì¼ë§ OCR: {tile_h}Ã—{tile_w}, ì˜¤ë²„ë© {overlap}px")
    
    # íƒ€ì¼ë³„ë¡œ ì²˜ë¦¬
    for y in range(0, h, step_h):
        for x in range(0, w, step_w):
            # íƒ€ì¼ ê²½ê³„ ì¡°ì •
            y_end = min(y + tile_h, h)
            x_end = min(x + tile_w, w)
            
            # ì‹¤ì œ íƒ€ì¼ í¬ê¸°
            actual_h = y_end - y
            actual_w = x_end - x
            
            # ë„ˆë¬´ ì‘ì€ íƒ€ì¼ì€ ê±´ë„ˆë›°ê¸°
            if actual_h < 100 or actual_w < 100:
                continue
                
            tile = image[y:y_end, x:x_end]
            
            print(f"      ğŸ“¦ íƒ€ì¼ ({x},{y})-({x_end},{y_end}) í¬ê¸°: {actual_w}Ã—{actual_h}")
            
            # íƒ€ì¼ë³„ OCR ìˆ˜í–‰ (ì´ë¯¸ ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ì´ë¯€ë¡œ ì§ì ‘ OCR)
            tile_result = ocr_tile_direct(tile, timeout=20)
            
            if tile_result and 'blocks' in tile_result:
                # ì¢Œí‘œë¥¼ ì „ì²´ ì´ë¯¸ì§€ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
                for block in tile_result['blocks']:
                    if 'bbox' in block:
                        bbox = block['bbox']
                        # íƒ€ì¼ ë‚´ ì¢Œí‘œë¥¼ ì „ì²´ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                        adjusted_bbox = [
                            bbox[0] + x,  # x1
                            bbox[1] + y,  # y1
                            bbox[2] + x,  # x2
                            bbox[3] + y   # y2
                        ]
                        block['bbox'] = adjusted_bbox
                        
                all_blocks.extend(tile_result['blocks'])
    
    # ì¤‘ë³µ ì œê±° ë° ë¸”ë¡ ë³‘í•© (ì˜¤ë²„ë© ì˜ì—­ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆìŒ)
    filtered_blocks = remove_overlapping_blocks(all_blocks)
    
    print(f"    âœ… íƒ€ì¼ë§ ì™„ë£Œ: {len(all_blocks)}ê°œ â†’ {len(filtered_blocks)}ê°œ ë¸”ë¡")
    
    # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë„ í•¨ê»˜ ë°˜í™˜
    return {"blocks": filtered_blocks, "processed_image": image}

def remove_overlapping_blocks(blocks, iou_threshold=0.3):
    """ì¤‘ë³µë˜ëŠ” ë¸”ë¡ ì œê±° ë° íƒ€ì¼ ê²½ê³„ì—ì„œ ì˜ë¦° ë¸”ë¡ ë³‘í•©"""
    if len(blocks) <= 1:
        return blocks
    
    # IoU ê³„ì‚° í•¨ìˆ˜
    def calculate_iou(box1, box2):
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        if x2 <= x1 or y2 <= y1:
            return 0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0
    
    # ë¸”ë¡ì„ yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)
    sorted_blocks = sorted(blocks, key=lambda b: (b.get('bbox', [0,0,0,0])[1], b.get('bbox', [0,0,0,0])[0]))
    
    filtered = []
    
    for i, block in enumerate(sorted_blocks):
        is_duplicate = False
        merged = False
        
        for j, existing_block in enumerate(filtered):
            if 'bbox' not in block or 'bbox' not in existing_block:
                continue
                
            iou = calculate_iou(block['bbox'], existing_block['bbox'])
            
            # 1. ì¤‘ë³µ ë¸”ë¡ ì œê±° (IoUê°€ ë†’ìœ¼ë©´ ì¤‘ë³µ)
            if iou > iou_threshold:
                # í…ìŠ¤íŠ¸ê°€ ë” ê¸´ ê²ƒì„ ì„ íƒ
                text1 = block.get('text', '')
                text2 = existing_block.get('text', '')
                
                if len(text1) > len(text2):
                    filtered[j] = block
                is_duplicate = True
                break
            
            # 2. íƒ€ì¼ ê²½ê³„ì—ì„œ ì˜ë¦° ë¸”ë¡ ë³‘í•© (IoUëŠ” ë‚®ì§€ë§Œ ì¸ì ‘í•œ ê²½ìš°)
            if should_merge_adjacent_blocks(block, existing_block):
                # ë¸”ë¡ ë³‘í•©
                merged_block = merge_two_blocks(block, existing_block)
                filtered[j] = merged_block
                merged = True
                
                # ë³‘í•© ë°©í–¥ í‘œì‹œ
                y1 = (existing_block['bbox'][1] + existing_block['bbox'][3]) / 2
                y2 = (block['bbox'][1] + block['bbox'][3]) / 2
                merge_type = "ì¢Œìš°" if abs(y1 - y2) < 30 else "ìƒí•˜"
                
                print(f"      ğŸ”— ë¸”ë¡ ë³‘í•©({merge_type}): '{existing_block.get('text', '')[:15]}' + '{block.get('text', '')[:15]}' â†’ '{merged_block.get('text', '')[:30]}'")
                break
        
        if not is_duplicate and not merged:
            filtered.append(block)
    
    return filtered

def should_merge_adjacent_blocks(block1, block2, max_gap=50, y_threshold=20):
    """ë‘ ë¸”ë¡ì´ íƒ€ì¼ ê²½ê³„ì—ì„œ ì˜ë¦° ê²ƒì¸ì§€ í™•ì¸ (xì¶• ë° yì¶•)"""
    bbox1 = block1.get('bbox', [])
    bbox2 = block2.get('bbox', [])
    
    if len(bbox1) != 4 or len(bbox2) != 4:
        return False
    
    # ê°™ì€ ì¹´í…Œê³ ë¦¬ì¸ì§€ í™•ì¸
    if block1.get('category') != block2.get('category'):
        return False
    
    # ì¼€ì´ìŠ¤ 1: xì¶•ìœ¼ë¡œ ì˜ë¦° ê²½ìš° (ê°™ì€ ì¤„ì—ì„œ ì¢Œìš°ë¡œ ì¸ì ‘)
    y1_center = (bbox1[1] + bbox1[3]) / 2
    y2_center = (bbox2[1] + bbox2[3]) / 2
    
    if abs(y1_center - y2_center) <= y_threshold:
        # xì¢Œí‘œê°€ ì¸ì ‘í•œì§€ í™•ì¸
        x_gap_right = bbox2[0] - bbox1[2]  # block2ê°€ block1ì˜ ì˜¤ë¥¸ìª½
        x_gap_left = bbox1[0] - bbox2[2]   # block1ì´ block2ì˜ ì˜¤ë¥¸ìª½
        
        if -max_gap < x_gap_right < max_gap or -max_gap < x_gap_left < max_gap:
            return True
    
    # ì¼€ì´ìŠ¤ 2: yì¶•ìœ¼ë¡œ ì˜ë¦° ê²½ìš° (ìœ„ì•„ë˜ë¡œ ì¸ì ‘, xì¶• ë²”ìœ„ ê²¹ì¹¨)
    x1_center = (bbox1[0] + bbox1[2]) / 2
    x2_center = (bbox2[0] + bbox2[2]) / 2
    
    # xì¶• ë²”ìœ„ê°€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸ (ì„¸ë¡œë¡œ ì´ì–´ì§„ ë¸”ë¡)
    x_overlap = min(bbox1[2], bbox2[2]) - max(bbox1[0], bbox2[0])
    x_min_width = min(bbox1[2] - bbox1[0], bbox2[2] - bbox2[0])
    
    # xì¶•ì´ 50% ì´ìƒ ê²¹ì¹˜ë©´ ê°™ì€ ì¹¼ëŸ¼ìœ¼ë¡œ ê°„ì£¼
    if x_overlap > x_min_width * 0.5:
        # yì¢Œí‘œê°€ ì¸ì ‘í•œì§€ í™•ì¸
        y_gap_bottom = bbox2[1] - bbox1[3]  # block2ê°€ block1ì˜ ì•„ë˜
        y_gap_top = bbox1[1] - bbox2[3]     # block1ì´ block2ì˜ ì•„ë˜
        
        if -max_gap < y_gap_bottom < max_gap or -max_gap < y_gap_top < max_gap:
            return True
    
    return False

def merge_two_blocks(block1, block2):
    """ë‘ ë¸”ë¡ì„ í•˜ë‚˜ë¡œ ë³‘í•© (xì¶• ë˜ëŠ” yì¶•)"""
    bbox1 = block1.get('bbox', [])
    bbox2 = block2.get('bbox', [])
    
    # ìƒˆë¡œìš´ bbox ê³„ì‚° (ë‘ ë¸”ë¡ì„ í¬í•¨í•˜ëŠ” ìµœì†Œ bbox)
    merged_bbox = [
        min(bbox1[0], bbox2[0]),  # x1
        min(bbox1[1], bbox2[1]),  # y1
        max(bbox1[2], bbox2[2]),  # x2
        max(bbox1[3], bbox2[3])   # y2
    ]
    
    # í…ìŠ¤íŠ¸ ë³‘í•© ë°©í–¥ íŒë‹¨
    y1_center = (bbox1[1] + bbox1[3]) / 2
    y2_center = (bbox2[1] + bbox2[3]) / 2
    
    # yì¶• ì°¨ì´ê°€ ì‘ìœ¼ë©´ xì¶•(ì¢Œìš°) ë³‘í•©, í¬ë©´ yì¶•(ìœ„ì•„ë˜) ë³‘í•©
    if abs(y1_center - y2_center) < 30:
        # xì¶• ë³‘í•© (ì¢Œìš°ë¡œ ì´ì–´ì§„ ê²½ìš°)
        if bbox1[0] < bbox2[0]:
            merged_text = block1.get('text', '') + block2.get('text', '')
        else:
            merged_text = block2.get('text', '') + block1.get('text', '')
    else:
        # yì¶• ë³‘í•© (ìœ„ì•„ë˜ë¡œ ì´ì–´ì§„ ê²½ìš°) - ì¤„ë°”ê¿ˆ ì¶”ê°€
        if bbox1[1] < bbox2[1]:  # block1ì´ ìœ„
            merged_text = block1.get('text', '') + '\n' + block2.get('text', '')
        else:  # block2ê°€ ìœ„
            merged_text = block2.get('text', '') + '\n' + block1.get('text', '')
    
    # ë³‘í•©ëœ ë¸”ë¡ ìƒì„±
    merged_block = block1.copy()
    merged_block['bbox'] = merged_bbox
    merged_block['text'] = merged_text
    
    # confidenceëŠ” í‰ê· ê°’ ì‚¬ìš©
    conf1 = block1.get('confidence', 0)
    conf2 = block2.get('confidence', 0)
    merged_block['confidence'] = (conf1 + conf2) / 2
    
    return merged_block

def ocr_with_dotsocr(image, timeout=300, max_retries=3):
    """DotsOCRë¡œ ë ˆì´ì•„ì›ƒë§Œ ì¶”ì¶œ (í…ìŠ¤íŠ¸ ì—†ìŒ, step1bì—ì„œ ì²˜ë¦¬)"""
    for attempt in range(max_retries):
        try:
            print(f"    ğŸ”„ ë ˆì´ì•„ì›ƒ ê°ì§€ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
            
            # ë ˆì´ì•„ì›ƒë§Œ ì¶”ì¶œ (í…ìŠ¤íŠ¸ ì—†ìŒ)
            result = extract_layout_only(image, timeout=timeout)
            if result:
                # ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜ (Picture í¬ë¡­ìš©)
                result['original_image'] = image
                return result
            else:
                print(f"    âŒ ë ˆì´ì•„ì›ƒ ì¶”ì¶œ ì‹¤íŒ¨")
                if attempt < max_retries - 1:
                    print(f"    ğŸ”„ {2}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(2)
                    continue
                return None
                
        except requests.exceptions.Timeout:
            print(f"    â° OCR íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
            if attempt < max_retries - 1:
                print(f"    ğŸ”„ {2}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(2)
                continue
            return None
        except requests.exceptions.ConnectionError:
            print(f"    ğŸ”Œ ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
            if attempt < max_retries - 1:
                print(f"    ğŸ”„ {2}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(2)
                continue
            return None
        except Exception as e:
            print(f"    âŒ OCR ì‹¤íŒ¨: {e}")
            if attempt < max_retries - 1:
                print(f"    ğŸ”„ {2}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(2)
                continue
            return None
    
    return None

def crop_picture_blocks(image, picture_blocks, doc_name, page_num, bge_model=None, clip_model=None):
    """Picture/Image ë¸”ë¡ë“¤ì„ í¬ë¡­í•˜ì—¬ ì €ì¥í•˜ê³  ì„ë² ë”© ìƒì„±"""
    cropped_images = []
    
    # í¬ë¡­ ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬ (ì„¸ì…˜ ë””ë ‰í† ë¦¬ ë‚´)
    crop_dir = OCR_DIR / doc_name / "crops"
    crop_dir.mkdir(parents=True, exist_ok=True)
    
    for i, block in enumerate(picture_blocks):
        bbox = block['bbox']
        block_id = f"picture_{i:03d}"
        
        # ë¸”ë¡ í¬ë¡­
        x1, y1, x2, y2 = bbox
        cropped_image = image[y1:y2, x1:x2]
        
        # íŒŒì¼ëª… ìƒì„±
        filename = f"{doc_name}_page{page_num:03d}_{block_id}.png"
        file_path = crop_dir / filename
        
        # í¬ë¡­ëœ ì´ë¯¸ì§€ ì €ì¥
        cv2.imwrite(str(file_path), cropped_image)
        
        # ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±
        caption = generate_image_caption(str(file_path))
        
        # ì„ë² ë”© ìƒì„±
        text_embedding = None
        image_embedding = None
        
        if bge_model:
            text_embedding = generate_text_embedding(caption, bge_model)
        
        if clip_model:
            image_embedding = generate_image_embedding(str(file_path), clip_model)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        cropped_info = {
            "block_id": block_id,
            "filename": filename,
            "file_path": str(file_path),
            "relative_path": f"ocr_results/{doc_name}/crops/{filename}",
            "bbox": bbox,
            "category": block['category'],
            "confidence": block.get('confidence', 0.9),
            "size": [x2-x1, y2-y1],
            "page_number": page_num,
            "document_name": doc_name,
            "caption": caption,
            "embeddings": {
                "text_embedding": text_embedding,
                "image_embedding": image_embedding,
                "embedding_models": {
                    "text_model": "dragonkue/bge-m3-ko" if bge_model else None,
                    "image_model": "clip-ViT-B-32" if clip_model else None
                }
            },
            "search_ready": bool(text_embedding or image_embedding)
        }
        
        cropped_images.append(cropped_info)
        print(f"      ğŸ“¦ {block_id}: {block['category']} â†’ {filename} (ì„ë² ë”©: {'âœ…' if cropped_info['search_ready'] else 'âŒ'})")
    
    return cropped_images

def draw_bbox_debug(image, blocks):
    """bbox ë””ë²„ê·¸ ì´ë¯¸ì§€ ìƒì„± (demo ìŠ¤íƒ€ì¼)"""
    debug_img = image.copy()
    
    # demoì™€ ë™ì¼í•œ ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒ ì •ì˜ (BGR í˜•ì‹)
    colors = {
        'Page-header': (0, 0, 255),     # ë¹¨ê°„ìƒ‰
        'Page-footer': (0, 0, 255),     # ë¹¨ê°„ìƒ‰  
        'Section-header': (0, 255, 0),  # ì´ˆë¡ìƒ‰
        'Table': (255, 0, 0),           # íŒŒë€ìƒ‰
        'Text': (0, 255, 255),          # ë…¸ë€ìƒ‰
        'Title': (255, 0, 255),         # ë§ˆì  íƒ€
        'List-item': (255, 255, 0),     # ì‹œì•ˆ
        'Picture': (128, 0, 128),       # ë³´ë¼ìƒ‰
        'Formula': (0, 128, 255),       # ì£¼í™©ìƒ‰
        'Caption': (128, 128, 128),     # íšŒìƒ‰
        'Footnote': (64, 64, 64)        # ì–´ë‘ìš´ íšŒìƒ‰
    }
    
    for i, block in enumerate(blocks):
        bbox = block.get('bbox', [0, 0, 100, 100])
        category = block.get('category', 'Text')
        
        if len(bbox) == 4:
            x1, y1, x2, y2 = bbox
            color = colors.get(category, (128, 128, 128))
            
            # demo ìŠ¤íƒ€ì¼: ë°˜íˆ¬ëª… ë°°ê²½ ì±„ìš°ê¸°
            overlay = debug_img.copy()
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
            cv2.addWeighted(overlay, 0.3, debug_img, 0.7, 0, debug_img)
            
            # í…Œë‘ë¦¬ ê·¸ë¦¬ê¸°
            cv2.rectangle(debug_img, (x1, y1), (x2, y2), color, 2)
            
            # ë¸”ë¡ ë²ˆí˜¸ì™€ ì¹´í…Œê³ ë¦¬ í‘œì‹œ (demo ìŠ¤íƒ€ì¼)
            label = f"{i:03d}:{category}"
            
            # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸°
            (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            cv2.rectangle(debug_img, (x1, y1-25), (x1+text_width+10, y1), color, -1)
            
            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            cv2.putText(debug_img, label, (x1+5, y1-8), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    return debug_img

def process_pdf(pdf_path, bge_model=None, clip_model=None, max_pages=None):
    """PDF íŒŒì¼ ì²˜ë¦¬ (ì›ë˜ ë°ëª¨ ë°©ì‹ + Picture í¬ë¡­ + ì„ë² ë”©)"""
    print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
    
    # ì„¸ì…˜ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (í•œ ë²ˆë§Œ)
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    doc_dir = OCR_DIR / pdf_path.stem / session_timestamp
    step1_dir = doc_dir / "step1_primary"
    step1_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"  ğŸ“ ì„¸ì…˜ ë””ë ‰í† ë¦¬: {doc_dir}")
    
    # ì„¸ì…˜ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ë‹¤ë¥¸ ë‹¨ê³„ì—ì„œ ì‚¬ìš©)
    session_info = {
        "document_name": pdf_path.stem,
        "session_timestamp": session_timestamp,
        "session_dir": str(doc_dir),
        "embedding_models": {
            "text_model": "dragonkue/bge-m3-ko" if bge_model else None,
            "image_model": "clip-ViT-B-32" if clip_model else None
        }
    }
    session_file = BASE_DIR / "current_session.json"
    with open(session_file, 'w', encoding='utf-8') as f:
        json.dump(session_info, f, ensure_ascii=False, indent=2)
    
    # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
    print("  ğŸ–¼ï¸ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜...")
    images = pdf_to_images(pdf_path)
    print(f"  âœ… {len(images)}í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ")
    
    # í˜ì´ì§€ ì œí•œ ì ìš©
    if max_pages and max_pages > 0:
        images = images[:max_pages]
        print(f"  ğŸ“„ ì²˜ìŒ {len(images)}í˜ì´ì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤")
    else:
        print(f"  ğŸ“„ ì „ì²´ {len(images)}í˜ì´ì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤")
    
    # ê° í˜ì´ì§€ OCR
    for page_num, image in images:
        print(f"  ğŸ“ í˜ì´ì§€ {page_num} OCR ì¤‘... ({image.shape})")
        
        # ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        if page_num > 1:
            time.sleep(2)
        
        ocr_result = ocr_with_dotsocr(image)
        if ocr_result:
            # ë ˆì´ì•„ì›ƒ ê²°ê³¼ íŒŒì‹±
            blocks = ocr_result.get("blocks", [])
            original_image = ocr_result.get("original_image", image)  # ì›ë³¸ ì´ë¯¸ì§€
            
            # Picture/Image ë¸”ë¡ ì°¾ê¸° ë° í¬ë¡­
            picture_blocks = []
            for block in blocks:
                category = block.get('category', '').lower()
                if category in ['picture', 'image']:
                    picture_blocks.append(block)
            
            # Picture ë¸”ë¡ í¬ë¡­ ë° ì €ì¥ (ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©, ì„ë² ë”© í¬í•¨)
            cropped_images = []
            if picture_blocks:
                print(f"    ğŸ–¼ï¸ Picture/Image ë¸”ë¡ {len(picture_blocks)}ê°œ í¬ë¡­ ì¤‘ (ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©)...")
                # ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ í¬ë¡­ (ì „ì²˜ë¦¬ ì—†ìŒ)
                cropped_images = crop_picture_blocks(original_image, picture_blocks, pdf_path.stem, page_num, bge_model, clip_model)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result_data = {
                "document_name": pdf_path.stem,
                "page_number": page_num,
                "timestamp": session_timestamp,
                "original_image_size": image.shape[:2],
                "pipeline_stage": "ë ˆì´ì•„ì›ƒ ê°ì§€ + ì›ë³¸ OCR",
                "preprocessing_applied": {
                    "layout_detection": "ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©",
                    "picture_crop": "ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš© (ì „ì²˜ë¦¬ ì—†ìŒ)",
                    "ocr_raw": "ì›ë³¸ ì´ë¯¸ì§€ OCR (DotsOCR)",
                    "ocr_preprocessed": "step1bì—ì„œ ì „ì²˜ë¦¬ OCR ì¶”ê°€ ì˜ˆì •"
                },
                "blocks": blocks,
                "picture_blocks": cropped_images,
                "total_picture_blocks": len(cropped_images),
                "embedding_models": {
                    "text_model": "dragonkue/bge-m3-ko" if bge_model else None,
                    "image_model": "clip-ViT-B-32" if clip_model else None
                }
            }
            
            # ê²°ê³¼ ì €ì¥ (ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
            output_path = step1_dir / f"page_{page_num:03d}.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            # bbox ë””ë²„ê·¸ ì´ë¯¸ì§€ ìƒì„± ë° ì €ì¥ (ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©)
            debug_image = draw_bbox_debug(original_image, result_data['blocks'])
            debug_path = step1_dir / f"page_{page_num:03d}_bbox_debug.png"
            cv2.imwrite(str(debug_path), debug_image)
            
            print(f"    âœ… {len(result_data['blocks'])}ê°œ ë ˆì´ì•„ì›ƒ ë¸”ë¡ ê°ì§€, {len(cropped_images)}ê°œ Picture í¬ë¡­ (ì›ë³¸) â†’ JSON + ë””ë²„ê·¸ ì´ë¯¸ì§€ ì €ì¥")
        else:
            print(f"    âŒ OCR ì‹¤íŒ¨")
    
    # ëª¨ë“  í˜ì´ì§€ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ combined.jsonìœ¼ë¡œ í†µí•©
    print("  ğŸ”— í˜ì´ì§€ ê²°ê³¼ í†µí•© ì¤‘...")
    combined_data = {
        "metadata": {
            "document_name": pdf_path.stem,
            "total_pages": len(images),
            "processed_pages": len(images),
            "preprocessing": {
                "gaussian_blur": {"kernel_size": 5, "sigma": 1.0},
                "unsharp_mask": {"radius": 1.2, "percent": 160, "threshold": 3},
                "upscale": {"scale_factor": 1.0, "method": "removed"}
            },
            "text_model": "dragonkue/bge-m3-ko" if bge_model else None,
            "image_model": "clip-ViT-B-32" if clip_model else None
        },
        "pages": []
    }
    
    for page_num, _ in images:
        page_file = step1_dir / f"page_{page_num:03d}.json"
        if page_file.exists():
            with open(page_file, 'r', encoding='utf-8') as f:
                page_data = json.load(f)
                combined_data["pages"].append(page_data)
    
    # combined.json ì €ì¥
    combined_file = step1_dir / "combined.json"
    with open(combined_file, 'w', encoding='utf-8') as f:
        json.dump(combined_data, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… í†µí•© íŒŒì¼ ìƒì„±: {combined_file}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='í†µí•© OCR íŒŒì´í”„ë¼ì¸')
    parser.add_argument('--max-pages', type=int, default=None, 
                        help='ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: ì „ì²´)')
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ Step 1: ë ˆì´ì•„ì›ƒ ê°ì§€ (í…ìŠ¤íŠ¸ ì—†ìŒ, PictureëŠ” ì›ë³¸ í¬ë¡­)")
    print("=" * 60)
    
    # ë””ë ‰í† ë¦¬ í™•ì¸
    OCR_DIR.mkdir(exist_ok=True)
    DEBUG_DIR.mkdir(exist_ok=True)
    
    # PDF íŒŒì¼ ì°¾ê¸°
    pdf_files = list(PDF_DIR.glob("*.pdf"))
    if not pdf_files:
        print("âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ê²½ë¡œ: {PDF_DIR}")
        return
    
    print(f"ğŸ“ ë°œê²¬ëœ PDF: {len(pdf_files)}ê°œ")
    
    # DotsOCR ì„œë²„ í™•ì¸
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        print("âœ… DotsOCR ì„œë²„ ì—°ê²°ë¨")
    except:
        print("âŒ DotsOCR ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”.")
        return
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
    import os
    enable_embeddings = os.getenv('ENABLE_EMBEDDINGS', 'false').lower() == 'true'
    
    if enable_embeddings:
        print("\nğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        try:
            bge_model, clip_model = load_embedding_models(load_clip=False)  # CLIPì€ ë‚˜ì¤‘ì— í•„ìš”í•  ë•Œ
        except Exception as e:
            print(f"   âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            bge_model, clip_model = None, None
    else:
        print("\nâ­ï¸ ì„ë² ë”© ê¸°ëŠ¥ ë¹„í™œì„±í™” (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
        print("   (ì„ë² ë”©ì„ í™œì„±í™”í•˜ë ¤ë©´: export ENABLE_EMBEDDINGS=true)")
        bge_model, clip_model = None, None
    
    if bge_model is None:
        print("âš ï¸ BGE ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ì„ë² ë”© ë¹„í™œì„±í™”")
    
    if clip_model is None:
        print("âš ï¸ CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ì´ë¯¸ì§€ ì„ë² ë”© ë¹„í™œì„±í™”")
    
    # PDF ì²˜ë¦¬
    for pdf_path in pdf_files:
        try:
            process_pdf(pdf_path, bge_model, clip_model, max_pages=args.max_pages)
        except Exception as e:
            print(f"âŒ {pdf_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ ë ˆì´ì•„ì›ƒ ê°ì§€ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼: {OCR_DIR}")
    if args.max_pages:
        print(f"ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: ìµœëŒ€ {args.max_pages}í˜ì´ì§€")
    else:
        print(f"ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: ì „ì²´ í˜ì´ì§€")
    print(f"ğŸ“ ë ˆì´ì•„ì›ƒ: ì›ë³¸ ì´ë¯¸ì§€ë¡œ ê°ì§€ (í…ìŠ¤íŠ¸ ì—†ìŒ)")
    print(f"ğŸ–¼ï¸ Picture í¬ë¡­: ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ í¬ë¡­ (ì „ì²˜ë¦¬ ì—†ìŒ)")
    print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸: BGE-m3-ko {'âœ…' if bge_model else 'âŒ'}, CLIP {'âœ…' if clip_model else 'âŒ'}")
    print(f"â­ï¸ ë‹¤ìŒ ë‹¨ê³„: step1bì—ì„œ í…ìŠ¤íŠ¸ ë¸”ë¡ ì „ì²˜ë¦¬ ì „/í›„ 2íšŒ OCR ìˆ˜í–‰")

def search_images_by_query(query, json_data, clip_model):
    """ì´ë¯¸ì§€ ê²€ìƒ‰ ì˜ˆì‹œ í•¨ìˆ˜"""
    if clip_model is None:
        print("âŒ CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        # ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = clip_model.encode(query)
        
        results = []
        for page_data in json_data.get('pages', []):
            for picture_block in page_data.get('picture_blocks', []):
                if picture_block.get('search_ready') and picture_block['embeddings'].get('image_embedding'):
                    # ìœ ì‚¬ë„ ê³„ì‚°
                    image_embedding = np.array(picture_block['embeddings']['image_embedding'])
                    similarity = np.dot(query_embedding, image_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(image_embedding))
                    
                    results.append({
                        'image_path': picture_block['file_path'],
                        'relative_path': picture_block['relative_path'],
                        'caption': picture_block['caption'],
                        'similarity': float(similarity),
                        'page_number': picture_block['page_number'],
                        'bbox': picture_block['bbox']
                    })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results
    
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

# ì‚¬ìš© ì˜ˆì‹œ
def example_image_search():
    """ì´ë¯¸ì§€ ê²€ìƒ‰ ì‚¬ìš© ì˜ˆì‹œ"""
    print("\nğŸ” ì´ë¯¸ì§€ ê²€ìƒ‰ ì˜ˆì‹œ:")
    print("ì§ˆë¬¸: 'AAì‚¬ì§„ì´ ë­ì•¼?'")
    print("â†’ í•´ë‹¹ ì´ë¯¸ì§€ íŒŒì¼ê³¼ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë°˜í™˜")
    print("â†’ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ, ìº¡ì…˜, í˜ì´ì§€ ì •ë³´ í¬í•¨")

if __name__ == "__main__":
    main()
