#!/usr/bin/env python3
"""
â‘ -b ì´ì¤‘ OCR (ì „ì²˜ë¦¬ ì „/í›„ 2íšŒ OCR)
ì…ë ¥: step1_primary/page_XXX.json (ë ˆì´ì•„ì›ƒë§Œ ìˆëŠ” íŒŒì¼)
ì²˜ë¦¬: í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ ì „ì²˜ë¦¬ ì „/í›„ 2íšŒ OCR â†’ ë‘ ê°’ ëª¨ë‘ ì €ì¥
ì¶œë ¥: step1b_dual/page_XXX.json (ocr_raw, ocr_preprocessed ë‘ í•„ë“œ ëª¨ë‘ í¬í•¨)
"""

import os
import json
import re
import requests
import base64
import cv2
import numpy as np
import time
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path
from PIL import Image, ImageEnhance, ImageFilter

# ì„¤ì •
DOTSOCR_URL = "http://localhost:8000/v1/chat/completions"
BASE_DIR = Path(__file__).parent.parent
OCR_DIR = BASE_DIR / "ocr_results"
CURRENT_SESSION_DIR = None

# ì› ì•ˆì˜ í•œê¸€ ë¬¸ì â†’ ì› ì•ˆì˜ ìˆ«ì ë§¤í•‘
CIRCLED_HANGUL_TO_NUMBER = {
    'ã‰®': 'â‘ ',  # ê°€ â†’ 1
    'ã‰¯': 'â‘¡',  # ë‚˜ â†’ 2
    'ã‰°': 'â‘¢',  # ë‹¤ â†’ 3
    'ã‰±': 'â‘£',  # ë¼ â†’ 4
    'ã‰²': 'â‘¤',  # ë§ˆ â†’ 5
    'ã‰³': 'â‘¥',  # ë°” â†’ 6
    'ã‰´': 'â‘¦',  # ì‚¬ â†’ 7
    'ã‰µ': 'â‘§',  # ì•„ â†’ 8
    'ã‰¶': 'â‘¨',  # ì â†’ 9
    'ã‰·': 'â‘©',  # ì°¨ â†’ 10
    'ã‰¸': 'â‘ª',  # ì¹´ â†’ 11
    'ã‰¹': 'â‘«',  # íƒ€ â†’ 12
    'ã‰º': 'â‘¬',  # íŒŒ â†’ 13
    'ã‰»': 'â‘­',  # í•˜ â†’ 14
}

def normalize_circled_characters(text):
    """ì› ì•ˆì˜ í•œê¸€ ë¬¸ìë¥¼ ì› ì•ˆì˜ ìˆ«ìë¡œ ì •ê·œí™”"""
    if not text:
        return text

    for hangul, number in CIRCLED_HANGUL_TO_NUMBER.items():
        text = text.replace(hangul, number)
    
    return text


def evaluate_ocr_consistency(raw_text, preprocessed_text):
    """ì›ë³¸ OCRê³¼ ì „ì²˜ë¦¬ OCRì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´ ë¶ˆì¼ì¹˜ ì—¬ë¶€ íŒë‹¨"""

    if not raw_text or not preprocessed_text:
        return {
            "similarity": 0.0,
            "overlap": 0.0,
            "mismatch": False,
        }

    def normalize(text):
        # ì˜ë¬¸/ìˆ«ì/í•œê¸€ ì¤‘ì‹¬ìœ¼ë¡œ ë¹„êµ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        return re.sub(r"[^0-9A-Za-zê°€-í£]", "", text)

    raw_norm = normalize(raw_text)
    pre_norm = normalize(preprocessed_text)

    if not raw_norm or not pre_norm:
        return {
            "similarity": 0.0,
            "overlap": 0.0,
            "mismatch": False,
        }

    similarity = SequenceMatcher(None, raw_norm, pre_norm).ratio()

    raw_set = set(raw_norm)
    pre_set = set(pre_norm)
    overlap = len(raw_set & pre_set) / max(len(raw_set | pre_set), 1)

    mismatch = similarity < 0.25 and overlap < 0.35 and len(pre_norm) > 6 and len(raw_norm) > 6

    return {
        "similarity": similarity,
        "overlap": overlap,
        "mismatch": mismatch,
    }

def apply_preprocessing(image):
    """ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© (í…ìŠ¤íŠ¸ OCRìš©)"""
    # OpenCV ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜
    if len(image.shape) == 3:
        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    else:
        pil_image = Image.fromarray(image)
    
    # 1. Gaussian Blur (5Ã—5)
    gaussian_kernel = ImageFilter.GaussianBlur(radius=2.5)
    blurred = pil_image.filter(gaussian_kernel)
    
    # 2. Unsharp Mask (r=1.2, p=160)
    unsharp_filter = ImageFilter.UnsharpMask(radius=1.2, percent=160, threshold=3)
    sharpened = blurred.filter(unsharp_filter)
    
    # 3. ì—…ìŠ¤ì¼€ì¼ë§ (2.8ë°°)
    original_size = sharpened.size
    scale_factor = 2.8
    new_size = (int(original_size[0] * scale_factor), int(original_size[1] * scale_factor))
    upscaled = sharpened.resize(new_size, Image.Resampling.LANCZOS)
    
    # PILì„ OpenCVë¡œ ë‹¤ì‹œ ë³€í™˜
    upscaled_cv = cv2.cvtColor(np.array(upscaled), cv2.COLOR_RGB2BGR)
    
    return upscaled_cv

def ocr_single_block(image, bbox, use_preprocessing=False, timeout=300):
    """ë‹¨ì¼ ë¸”ë¡ OCR ìˆ˜í–‰"""
    try:
        # bboxë¡œ ë¸”ë¡ í¬ë¡­
        x1, y1, x2, y2 = bbox
        cropped = image[y1:y2, x1:x2]
        
        if cropped.size == 0:
            return None
        
        # ì „ì²˜ë¦¬ ì ìš© ì—¬ë¶€
        if use_preprocessing:
            cropped = apply_preprocessing(cropped)
        
        # ì´ë¯¸ì§€ ì¸ì½”ë”©
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 85]
        _, buffer = cv2.imencode('.jpg', cropped, encode_param)
        image_data = base64.b64encode(buffer).decode('utf-8')
        
        # DotsOCR ìš”ì²­
        response = requests.post(DOTSOCR_URL, json={
            "model": "model",
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}},
                    {"type": "text", "text": """Extract all text from this image in reading order. Output ONLY the extracted text, without any formatting, explanation, or translation."""}
                ]
            }],
            "max_tokens": 2048,
            "temperature": 0.0
        }, timeout=timeout)
        
        if response.status_code == 200:
            result = response.json()
            text = result['choices'][0]['message']['content'].strip()
            return text
        else:
            return None
            
    except Exception as e:
        print(f"      âŒ OCR ì˜¤ë¥˜: {e}")
        return None

def process_page(page_data, original_image):
    """í˜ì´ì§€ë³„ ì´ì¤‘ OCR ì²˜ë¦¬"""
    page_num = page_data['page_number']
    blocks = page_data.get('blocks', [])
    
    print(f"  ğŸ“„ í˜ì´ì§€ {page_num}: {len(blocks)}ê°œ ë¸”ë¡ ì²˜ë¦¬ ì¤‘...")
    
    text_blocks_count = 0
    picture_blocks_count = 0
    
    for i, block in enumerate(blocks):
        category = block.get('category', '').lower()
        bbox = block.get('bbox', [])
        
        if len(bbox) != 4:
            continue
        
        # Picture/Image ë¸”ë¡ì€ ê±´ë„ˆëœ€ (ì´ë¯¸ step1ì—ì„œ ì²˜ë¦¬ë¨)
        if category in ['picture', 'image']:
            picture_blocks_count += 1
            print(f"    [{i+1:03d}] {category:15s} - ê±´ë„ˆëœ€ (step1ì—ì„œ ì²˜ë¦¬ë¨)")
            continue
        
        # í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ ì „ì²˜ë¦¬ OCR ìˆ˜í–‰ (ì›ë³¸ì€ step1ì—ì„œ ì´ë¯¸ ì¶”ì¶œë¨)
        print(f"    [{i+1:03d}] {category:15s} - ì „ì²˜ë¦¬ OCR ìˆ˜í–‰ ì¤‘...")
        
        # 1. ì›ë³¸ OCRì€ step1ì—ì„œ ì¶”ì¶œëœ ê²ƒ ì‚¬ìš©
        ocr_raw = block.get('ocr_raw', '')
        print(f"          1ï¸âƒ£ ì›ë³¸ OCR (step1): '{(ocr_raw[:30] + '...') if len(ocr_raw) > 30 else ocr_raw}'")
        
        # 2. ì „ì²˜ë¦¬ í›„ OCRë§Œ ì¶”ê°€ë¡œ ìˆ˜í–‰
        print(f"          2ï¸âƒ£ ì „ì²˜ë¦¬ ì´ë¯¸ì§€ OCR...")
        ocr_preprocessed = ocr_single_block(original_image, bbox, use_preprocessing=True)
        
        # ì› ì•ˆì˜ í•œê¸€ ë¬¸ì ì •ê·œí™” ì ìš©
        ocr_raw = normalize_circled_characters(ocr_raw) if ocr_raw else ""
        ocr_preprocessed = normalize_circled_characters(ocr_preprocessed) if ocr_preprocessed else ""
        
        # ì›ë³¸/ì „ì²˜ë¦¬ ê²°ê³¼ ë¹„êµ ë° ìƒíƒœ ê¸°ë¡
        status = {
            "state": "empty" if not ocr_preprocessed else "unknown",
            "similarity": 0.0,
            "overlap": 0.0,
        }

        if ocr_preprocessed:
            consistency = evaluate_ocr_consistency(ocr_raw, ocr_preprocessed)
            status.update({
                "similarity": round(consistency["similarity"], 3),
                "overlap": round(consistency["overlap"], 3),
            })

            if consistency["mismatch"] and ocr_raw:
                status["state"] = "discarded_mismatch"
                status["discarded_preview"] = (ocr_preprocessed[:40] + '...') if len(ocr_preprocessed) > 40 else ocr_preprocessed
                print(
                    f"          âš ï¸ ì „ì²˜ë¦¬ OCR ë¶ˆì¼ì¹˜ â†’ íê¸° (ìœ ì‚¬ë„ {consistency['similarity']:.2f}, ì¤‘ë³µë¹„ìœ¨ {consistency['overlap']:.2f})"
                )
                ocr_preprocessed = ""
            else:
                status["state"] = "kept"
        else:
            status["state"] = "empty"

        # ê²°ê³¼ ì €ì¥
        block['ocr_raw'] = ocr_raw
        block['ocr_preprocessed'] = ocr_preprocessed
        block['ocr_preprocessed_status'] = status
        block['ocr_pending'] = False

        # ë¯¸ë¦¬ë³´ê¸°
        raw_preview = (ocr_raw[:30] + '...') if ocr_raw and len(ocr_raw) > 30 else (ocr_raw or "(ì—†ìŒ)")
        prep_preview = (ocr_preprocessed[:30] + '...') if ocr_preprocessed and len(ocr_preprocessed) > 30 else (ocr_preprocessed or "(ì—†ìŒ)")

        print(f"          âœ… ì›ë³¸: {raw_preview}")
        print(f"          âœ… ì „ì²˜ë¦¬: {prep_preview}")
        
        text_blocks_count += 1
        
        # API ê³¼ë¶€í•˜ ë°©ì§€
        time.sleep(0.5)
    
    print(f"    âœ… í…ìŠ¤íŠ¸ ë¸”ë¡ {text_blocks_count}ê°œ ì´ì¤‘ OCR ì™„ë£Œ, Picture {picture_blocks_count}ê°œ ê±´ë„ˆëœ€")
    
    return page_data

def main():
    print("=" * 60)
    print("ğŸ”„ Step 1b: ì´ì¤‘ OCR (ì „ì²˜ë¦¬ ì „/í›„ 2íšŒ)")
    print("=" * 60)
    
    # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì½ê¸°
    session_file = BASE_DIR / "current_session.json"
    if not session_file.exists():
        print("âŒ í˜„ì¬ ì„¸ì…˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. step1ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    with open(session_file, 'r', encoding='utf-8') as f:
        session_info = json.load(f)
    
    global CURRENT_SESSION_DIR
    CURRENT_SESSION_DIR = Path(session_info['session_dir'])
    
    # step1 ê²°ê³¼ íŒŒì¼ ë¡œë“œ
    step1_dir = CURRENT_SESSION_DIR / "step1_primary"
    page_files = sorted(step1_dir.glob("page_*.json"))
    
    if not page_files:
        print(f"âŒ step1 ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {step1_dir}")
        return
    
    print(f"ğŸ“‚ ë°œê²¬ëœ í˜ì´ì§€: {len(page_files)}ê°œ")
    
    # DotsOCR ì„œë²„ í™•ì¸
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        print("âœ… DotsOCR ì„œë²„ ì—°ê²°ë¨")
    except:
        print("âŒ DotsOCR ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    step1b_dir = CURRENT_SESSION_DIR / "step1b_dual"
    step1b_dir.mkdir(parents=True, exist_ok=True)
    
    # PDF ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œë¥¼ ìœ„í•œ ì •ë³´
    import fitz
    pdf_name = session_info.get('document_name', 'unknown')
    pdf_path = BASE_DIR / "pdf_in" / f"{pdf_name}.pdf"
    
    if not pdf_path.exists():
        print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return
    
    # PDF ì—´ê¸°
    doc = fitz.open(pdf_path)
    
    # í˜ì´ì§€ë³„ ì²˜ë¦¬
    for page_file in page_files:
        with open(page_file, 'r', encoding='utf-8') as f:
            page_data = json.load(f)
        
        page_num = page_data['page_number']
        
        # PDFì—ì„œ í•´ë‹¹ í˜ì´ì§€ ì´ë¯¸ì§€ ì¶”ì¶œ
        page = doc.load_page(page_num - 1)  # 0-based index
        mat = fitz.Matrix(300/72, 300/72)  # 300 DPI
        pix = page.get_pixmap(matrix=mat)
        img_data = pix.tobytes("png")
        
        # OpenCVë¡œ ë³€í™˜
        nparr = np.frombuffer(img_data, np.uint8)
        original_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        # ì´ì¤‘ OCR ì²˜ë¦¬
        processed_page = process_page(page_data, original_image)
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        processed_page['pipeline_stage'] = 'ì´ì¤‘ OCR (ì „ì²˜ë¦¬ ì „/í›„)'
        processed_page['dual_ocr_applied'] = True
        processed_page['preprocessing_info'] = {
            'gaussian_blur': '5x5',
            'unsharp_mask': 'r=1.2, p=160',
            'upscale': 'x2.8'
        }
        
        # ê²°ê³¼ ì €ì¥
        output_path = step1b_dir / f"page_{page_num:03d}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(processed_page, f, ensure_ascii=False, indent=2)
        
        print(f"  ğŸ’¾ ì €ì¥: {output_path.name}")
    
    doc.close()
    
    # í†µí•© íŒŒì¼ ìƒì„±
    print("\nğŸ”— í˜ì´ì§€ ê²°ê³¼ í†µí•© ì¤‘...")
    combined_data = {
        "metadata": {
            "document_name": pdf_name,
            "total_pages": len(page_files),
            "pipeline_stage": "ì´ì¤‘ OCR",
            "timestamp": datetime.now().isoformat(),
            "ocr_methods": {
                "ocr_raw": "ì›ë³¸ ì´ë¯¸ì§€ (ì „ì²˜ë¦¬ ì—†ìŒ)",
                "ocr_preprocessed": "ì „ì²˜ë¦¬ ì´ë¯¸ì§€ (Gaussian + Unsharp + Upscale)"
            }
        },
        "pages": []
    }
    
    for page_file in sorted(step1b_dir.glob("page_*.json")):
        with open(page_file, 'r', encoding='utf-8') as f:
            page_data = json.load(f)
            combined_data["pages"].append(page_data)
    
    combined_file = step1b_dir / "combined.json"
    with open(combined_file, 'w', encoding='utf-8') as f:
        json.dump(combined_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… í†µí•© íŒŒì¼ ìƒì„±: {combined_file}")
    
    print(f"\nğŸ‰ ì´ì¤‘ OCR ì™„ë£Œ!")
    print(f"ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: {len(page_files)}ê°œ")
    print(f"ğŸ“ ê²°ê³¼: {step1b_dir}")
    print(f"ğŸ“Š ê° í…ìŠ¤íŠ¸ ë¸”ë¡ì— ocr_raw, ocr_preprocessed ë‘ ê°’ ì €ì¥ë¨")
    print(f"â­ï¸ ë‹¤ìŒ ë‹¨ê³„: step6ì—ì„œ LLMì´ ë‘ ê°’ ë¹„êµí•˜ì—¬ ìµœì  ê°’ ì„ íƒ")

if __name__ == "__main__":
    main()

